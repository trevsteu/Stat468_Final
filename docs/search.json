[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat468 Final Project",
    "section": "",
    "text": "1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Index</span>"
    ]
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Stat468 Final Project",
    "section": "1.1 Abstract",
    "text": "1.1 Abstract\nIn the days of and leading up to the 2025 NHL Entry Draft there were a total of 18 trades which only included draft picks. This report aims to use player contribution data to determine the relative value of selections in the NHL Entry Draft. To do this, data will be imported from Hockey Reference, and several potential models will be fit to the data with the goal of predicting the value of each pick based on historical outcomes. The R Shiny app component of this project allows users to interactively check the fairness of potential trades.\nKnowing the relative value of picks allows NHL teams to both determine whether they should accept trade offers they have received as well as propose favourable trades to other teams. That is, this project makes contributions to both asset valuation as well as trade analysis, both in the context of selections in the NHL Entry Draft.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Index</span>"
    ]
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Stat468 Final Project",
    "section": "1.2 Data",
    "text": "1.2 Data\nThe data used by this report is imported from Hockey Reference, which has data on the NHL Draft and player games played and point share counts dating back to 1963. Each row on Hockey Reference is one player selected, and the columns included on the site are:\n\nOverall: the selection number where the player was selected.\nTeam: the team that selected the player.\nName, Nat, Pos, Age: the player’s name, nationality, position, at age at the time of the draft.\nTo: the last year a player played in the NHL. For players who never played in the NHL this will be the empty string, for those who are still playing it will be 2025.\nAmateur Team: the team the player was drafted from (confusingly this could be a pro European team).\nGP, G, A, PTS, +/-, PIM: the player’s career games played, goals, assists, points (goals plus assists), plus minus, and penalty minutes. For players who never played in the NHL this will be empty strings. For goalies, this GP column will match the next GP column for goalies, and values are not necessarily the empty string for goalies (eg a goalie could have gotten an assist). Note that I will abbreviate games played to GP in this report.\nGP, W, L, T/O, SV%, GAA: the goalie’s career games played, wins, losses, ties plus overtime losses, save percentage, and goals against average. For goalies who never played in the NHL and all skaters, these columns will all be empty strings.\nPS: the player’s estimated point shares, or points added to their team over the course of their career (here we mean points in the standings, NOT goals and assists). There is more info on point shares here. Note that I abbreviate point share to PS in this report.\n\nAll stats listed are for regular season games only, as we do not want to put players who played on bad teams at a disadvantage more than they already are (it’s harder to get a high PS if your team rarely wins). Note that we will only use a subset of the years between 1963 and 2025 and of the attributes listed above, as will be explained in the Import and Tidy chapters.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Index</span>"
    ]
  },
  {
    "objectID": "index.html#constraints",
    "href": "index.html#constraints",
    "title": "Stat468 Final Project",
    "section": "1.3 Constraints",
    "text": "1.3 Constraints\nThere are a number of technical and practical constraints at play. Here are some of them:\n\nThere have only been 63 drafts in NHL history, but drafts which occurred too long ago are likely not relevant and drafts which occurred too recently are difficult to evaluate. This will be discussed further in the Import chapter.\nPlayers drafted earlier in a draft (ie with a better pick) typically get more opportunities than players selected in the later rounds. In particular, teams often fall victim to the sunk cost fallacy because scouts and management look bad when players who they invested a high pick into don’t ever play in a game for the team. Accounting for this is very difficult or even impossible, and we will not attempt to remedy it.\nEstimating the value of a player’s career is not a trivial task. One approach we will use is called point share. Point share is calculated by Hockey Reference and incorperates several stats, but it is still not a perfect metric as it can still be dependent on external factors, such as the quality of the player’s team the opportunities the player was given. Additionally, we will have to estimate what the career point share will be for players who have not yet retired. This issue, and an adjustment for it, will be discussed further in the Transform step.\nWe are interested in using historical data to predict the value of a draft pick from the team’s perspective. One slight problem with this is that the value of the pick from the team’s perspective depends on how long the player stayed on their team and what (if anything) the team got when the player left the team (via trade, free agency, or retirement). We will ignore this because it is nearly impossible to take these factors into account.\nEvery draft has strong portions and weak portions. For example, one draft might have a very strong second round (by that we mean the prospects drafted in the second round in that particular draft are of higher quality than those typically drafted in the second round). Though this seems like an obvious point, it is crucial to mention because it is a significant asterisk on this report, which will assume all drafts have equal value structures (ie the quality of a prospect #27 overall of draft A is the same as the quality of a prospect drafted at # 27 of draft B).\nSimilarly, if a team really likes a player he might be worth more than the historical value of the pick.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Index</span>"
    ]
  },
  {
    "objectID": "question.html",
    "href": "question.html",
    "title": "2  Question",
    "section": "",
    "text": "2.1 Approach\nDepending on the context, there can be countless approaches one could take to define the relative value of each selection of an NHL Entry Draft. As one example, one could look at prior trades and then define the value of a pick to be what it has historically been traded for (the “market rate”), as was done by Isack (2022) and Tulsky (2013). For example, this approach might say that pick \\(i\\) is typically traded for pick \\(j\\) and pick \\(k\\). In contrast, this report will take the approach of defining the value of selection \\(i\\) to be the career contribution of the player drafted at \\(i^{\\text{th}}\\) overall. This matches the definition chosen by Moreau (2020), and it is preferred in this context because we wish to find opportunities for general managers to make trades that help their team get a higher (expected)\nThis report will estimate value of pick \\(i\\) (ie find \\(\\hat v_i\\)) by fitting several models incorporating the PS and GP data by the players previously selected at pick \\(i\\) (and the selections “close” to \\(n\\)). We will consider two different methods of measuring the quality of a player (and thus the quality of a pick), and for each we will fit a linear and non-linear regression model.\nThe first two methods of evaluating player quality are similar to approaches taken by Moreau (2022), and will take the average PS (or GP) of all players taken at pick \\(i\\). Next, will consider a similar metric which adjusts the PS values for active players based on what we estimate their PS will be in the remainder of their career. The final approach is from Luo (2024) and will define an indicator random variable \\(Z_{ij} = 1\\) iff the player selected at pick \\(i\\) of draft \\(j\\) has played or will play in 200 career NHL games. We then take the average value of this indicator for each \\(i\\), which will be our estimate of the probability that a player selected at pick \\(i\\) will play in 200 NHL games. Thus in total we have four measures of the quality of players drafted at pick \\(i\\) (mean PS, mean GP, adjusted mean PS, and mean proportion of players who play in \\(\\ge\\) 200 games.\nTo maintain some consistency with previous related work, this report will define \\(v_i\\) to be “true” value of the \\(i^{\\text{th}}\\) selection, and this value will be in terms of unitless “points”, which are aimed at making comparing the value of picks easier. That is, instead of saying \\(v_i = \\frac{a}{b} v_1\\) for \\(i &gt; 1\\) where \\(a &lt; b\\), we will say \\(v_1 = 1000\\) points, \\(v_i = c\\) points for \\(0 &lt; c &lt; 1000\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Question</span>"
    ]
  },
  {
    "objectID": "question.html#considerations",
    "href": "question.html#considerations",
    "title": "2  Question",
    "section": "2.2 Considerations",
    "text": "2.2 Considerations\nWe will fit 7 models in the Model chapter, so we need some criteria for what a feasible model looks like. We define a feasible model to be a model which satisfies both of the following:\n\n\\(v_i &gt; v_{i+k}\\) for all \\(i, k \\in \\mathbb Z^+\\)\n\nThis ensures later draft picks are not considered more valuable than picks earlier in the draft. This should intuitively make sense because the players available at pick \\(i+k\\) are a proper subset of the players available at pick \\(i\\), so there is no reason for a later pick to be more valuable in a trade context than an earlier pick.\n\n\\(v_i &gt; 0\\) for all \\(i \\in \\mathbb Z^+\\).\n\nEvery pick is worth a positive number of points, since there is no real negative impact to picking a bad player (other than the opportunity cost of the “wasted” draft pick).\n\n\nWhile we will not include this in the definition of a feasible model, it is common knowledge in ice hockey circles (and confirmed by the previous work listed below) that NHL draft picks do not decrease in value linearly. In particular, \\(v_i\\) decreases quickly in \\(i\\), so the difference in value pick 1 and 30 is much greater than between pick 101 and 130 (ie \\(v_1 - v_{30} &gt;&gt;&gt; v_{101} - v_{130}\\)). In the Model chapter we will fit this model and show that it is not appropriate, before quickly moving on to non-linear models.\nNote that if picks did decrease linearly in value linearly then it would be very easy to create a model of draft pick value since we would have\n\\[ v_1 = v_2 + c = v_3 + 2c = ... = v_{224} + 223c \\]where \\(c &gt; 0\\), meaning we would only have to find the value of \\(c\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Question</span>"
    ]
  },
  {
    "objectID": "question.html#previous-work",
    "href": "question.html#previous-work",
    "title": "2  Question",
    "section": "2.3 Previous Work",
    "text": "2.3 Previous Work\nSome work in this area has been done before, such as:\n\nValuation of NHL Draft Picks using Functional Data Analysis\nImproving NHL Draft Outcome Predictions using Scouting Reports\nExamining the value of NHL Draft picks\nNHL draft: What does it cost to trade up?\n\nThis report will most closely follow the work done in the first two papers listed. As an interesting aside, Eric Tulsky, who wrote the last article listed above in 2013, was hired as General Manager of the Carolina Hurricanes in 2024.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Question</span>"
    ]
  },
  {
    "objectID": "import.html",
    "href": "import.html",
    "title": "3  Import",
    "section": "",
    "text": "3.1 Setup\nWe install and load the necessary packages.\nCode\n# renv::install(\"rvest\")\n# renv::install(\"stringr\")\n# renv::install(\"tidyverse\")\n# renv::install(\"janitor\")\n# renv::install(\"gt\")\n# renv::install(\"reactable\")\n\nlibrary(rvest)\nlibrary(stringr)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(gt)\nlibrary(reactable)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Import</span>"
    ]
  },
  {
    "objectID": "import.html#introduction",
    "href": "import.html#introduction",
    "title": "3  Import",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\nIn the import step we import the data required for this report. As mentioned before, we will be importing data from Hockey Reference. Before we import any data, it’s important to consider which and how many years we want to include in this analysis.\nSince the NHL has changed dramatically over the years, care must be taken to ensure we do not include drafts from too long ago. There are two primary concerns with including data from too many years ago. The first concern is that requirements for a player to be eligible to be drafted in the first place have changed since the first NHL Draft in 1963, and the second concern is that teams have likely changed their drafting approach and strategy over time.\nOne example of the first concern is that in 1979 the NHL began allowing players who had already played professionally for non-NHL teams to enter the draft. This meant that players who played professionally in Europe or in the World Hockey Association, which folded in 1979. Thus the level of talent available to be drafted would generally be higher in drafts from 1979 onward (there were more players eligible to be picked). Thus if we included drafts prior to 1979 we would probably underestimate the value of later selections, because selections later in a draft would likely have more talent available. There have also been changes in regards to the ages of players who are eligible, currently players need to be between 18-20 as of September 15th of the Draft’s year.\nIn regards to the second concern, teams may have become better at evaluating prospects as more advanced statistics have been developed, meaning that there are likely fewer late round draft “steals” in the 2020s than there were in the 1980s. Thus including drafts from the 1980s would skew our calculations, and we would likely overestimate the value of later picks, since the late round steals potentially may have been drafted sooner if the teams of the 1980s had the resources available to teams today. This would make our model a poor predictor of draft pick value for drafts occurring in the 2020s.\nA clear example of the evolving draft strategies which could impact our conclusion is the fact that it is becoming increasingly rare for teams to draft older prospects, especially with high picks. For example, 9 of the first 10 picks in the 1980 NHL Entry Draft were 19 or 20 years old at the time of the draft. In contrast, the first 19 or 20 year old was not selected until the 49th pick of the 2025 NHL Entry Draft. Though the impact of this change is not clear, it demonstrates a clear shift in drafting strategy, and it would be expected that the evaluation of prospects has changed over time too, which would change the relative value of picks (teams being more efficient drafters means later picks would be less valuable). With both of these concerns in mind, we clearly need to be careful about including drafts from too long ago.\nThat being said, players drafted in recent years have not had sufficient time to contribute to their teams, so we should not include drafts from too recently either. Ideally, we would wait until all players from a draft class have retired before including it in our analysis. Practically speaking, this is not feasible since players can have very long careers (for example, Alex Ovechkin was drafted in 2004 and is still playing) which would force us to include older drafts to maintain the same sample size, which is also not ideal as explained above.\nAnother consideration is that the formula for calculating a skater’s PS (point share; the metric we will use for predicting pick value) changed in either the 1997-1998 or 1998-1999 season. There is conflicting info on what year it changed; this link says it changed in 1998-1999 because time on ice data was not available until 1998-1999, however the page of 1997-1998 data has time on ice data. In the seasons where time on ice data was not available, games played was used instead. To maintain consistency, we would prefer to minimize the number of players in our dataset who played before the 1998-1999 season, since those seasons were definitely under the old PS formula. The PS formula for goalies has been the same since the 1983-1984 season, so it is not an issue.\nTaking these factors into consideration, we make the somewhat arbitrary decision to use the 25 drafts between and 1996 and 2020 (inclusive). We don’t have the code to check this yet, but at the end of this chapter we will find the number of games played by players in our dataset under the old PS formula. Less than 0.2% of games in our dataset were played under the old PS formula and thus these games are unlikely to meaningfully impact our conclusion. However, this percentage would get progressively worse if we were to include more from prior to the 1997-1998 season. The dates included are similar to those included by Moreau in his paper, which was published in 2020 and included players drafted between 1982 and 2016, inclusive.\nOne final note is that, as we will see in the Visualize chapter, a significant portion of the players in our dataset are still active, so we may want to adjust for this. This will be discussed further in the Transform chapter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Import</span>"
    ]
  },
  {
    "objectID": "import.html#importing-the-data",
    "href": "import.html#importing-the-data",
    "title": "3  Import",
    "section": "3.3 Importing the Data",
    "text": "3.3 Importing the Data\nWe start off by creating a function to import data from Hockey Reference.\n\n\nCode\nstart_year &lt;- 1996\nend_year &lt;- 2020\n\nimport_draft &lt;- function(year){\n  url &lt;- str_glue(\"https://www.hockey-reference.com/draft/NHL_{year}_entry.html\")\n  html &lt;- read_html(url)\n  Sys.sleep(5) # to avoid getting rate limited\n  draft_year_table &lt;- html |&gt; \n    html_element(\"table\") |&gt; \n    html_table() |&gt; \n    janitor::row_to_names(1) |&gt; \n    janitor::clean_names()\n  draft_year_table\n}\n\ngt(head(import_draft(start_year), 10))\n\n\n\n\n\n\n\n\noverall\nteam\nplayer\nnat\npos\nage\nto\namateur_team\ngp\ng\na\npts\nx\npim\ngp_2\nw\nl\nt_o\nsv_percent\ngaa\nps\n\n\n\n\n1\nOttawa Senators\nChris Phillips\nCA\nD\n18\n2015\nPrince Albert Raiders (WHL)\n1179\n71\n217\n288\n68\n756\n\n\n\n\n\n\n64.6\n\n\n2\nSan Jose Sharks\nAndrei Zyuzin\nRU\nD\n18\n2008\nSalavat Yulaev Ufa (Russia)\n496\n38\n82\n120\n-40\n446\n\n\n\n\n\n\n25.8\n\n\n3\nNew York Islanders\nJ.P. Dumont\nCA\nRW\n18\n2011\nVal-d'Or Foreurs (QMJHL)\n822\n214\n309\n523\n-2\n364\n\n\n\n\n\n\n56.6\n\n\n4\nWashington Capitals\nAlexandre Volchkov\nRU\nC\n18\n2000\nBarrie Colts (OHL)\n3\n0\n0\n0\n-2\n0\n\n\n\n\n\n\n-0.1\n\n\n5\nDallas Stars\nRic Jackman\nCA\nD\n18\n2007\nSoo Greyhounds (OHL)\n231\n19\n58\n77\n-54\n166\n\n\n\n\n\n\n8.8\n\n\n6\nEdmonton Oilers\nBoyd Devereaux\nCA\nC\n18\n2009\nKitchener Rangers (OHL)\n627\n67\n112\n179\n5\n205\n\n\n\n\n\n\n12.5\n\n\n7\nBuffalo Sabres\nErik Rasmussen\nUS\nLW/C\n19\n2007\nMinnesota (WCHA)\n545\n52\n76\n128\n5\n305\n\n\n\n\n\n\n9.2\n\n\n8\nBoston Bruins\nJohnathan Aitken\nCA\nD\n18\n2004\nMedicine Hat Tigers (WHL)\n44\n0\n1\n1\n-12\n70\n\n\n\n\n\n\n0.0\n\n\n9\nAnaheim Ducks\nRuslan Salei\nBY\nD\n21\n2011\nLas Vegas Thunder (IHL)\n917\n45\n159\n204\n-25\n1065\n\n\n\n\n\n\n46.9\n\n\n10\nNew Jersey Devils\nLance Ward\nCA\nD\n18\n2004\nRed Deer Rebels (WHL)\n209\n4\n12\n16\n-30\n391\n\n\n\n\n\n\n2.7\n\n\n\n\n\n\n\nWe compare the first 10 rows of the 1996 draft table shown above with the table on Hockey Reference, and it seems that the function we created does what we want it to do.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Import</span>"
    ]
  },
  {
    "objectID": "import.html#evaluating-our-choice-of-drafts",
    "href": "import.html#evaluating-our-choice-of-drafts",
    "title": "3  Import",
    "section": "3.4 Evaluating our Choice of Drafts",
    "text": "3.4 Evaluating our Choice of Drafts\nWe can now find the number of skaters who played under the old PS formula. As mentioned earlier, it is not clear whether the PS formula changed in the 1997-1998 or 1998-1999 season, so we will check both. Note that players drafted in 1998 or later cannot have played in NHL games in the 1996-1997 or 1997-1998 seasons, so we only need to check players drafted in 1996 and 1997. Also, Hockey Reference URLs use the year the season ended in, so to get stats for the 1996-1997 season the year will be 1997. We also need to be careful when doing analysis with uncleaned data, but this will not be an issue for the short checks we are doing here.\n\n\nCode\ndraft_1996_1997 &lt;- rbind(import_draft(1996), import_draft(1997)) |&gt; \n  filter(pos != \"G\") |&gt; # the ps formula for goalies is the same for our entire dataset\n  select(\"player\") # we just want to compare names\n\nplayer_stats &lt;- function(year){\n  url &lt;- str_glue(\"https://www.hockey-reference.com/leagues/NHL_{year}_skaters.html\")\n  html &lt;- read_html(url)\n  Sys.sleep(5) # to avoid getting rate limited\n  stats_table &lt;- html |&gt; \n    html_element(\"table\") |&gt; \n    html_table() |&gt; \n    janitor::row_to_names(1) |&gt; \n    janitor::clean_names() |&gt; \n    type.convert() |&gt; \n    select(player, gp) |&gt; \n    group_by(player) |&gt; \n    # players who played for n &gt; 1 teams get listed n+1 times; this fixes it \n    summarize(gp = max(gp), .groups = \"drop\")  \n  stats_table \n}\n\nplayer_stats_1996_1997 &lt;- full_join(player_stats(1997), # year is end of season\n                                    player_stats(1998), # year is end of season\n                                    by = join_by(player))\n\nold_ps_players &lt;- player_stats_1996_1997 |&gt; \n  rename(gp_1997 = gp.x, gp_1998 = gp.y) |&gt; \n  filter(player %in% draft_1996_1997$player) |&gt;\n  mutate(gp_1997 = coalesce(gp_1997, 0), \n         gp_1998 = coalesce(gp_1998, 0))\n\nreactable(arrange(old_ps_players, player))\n\n\n\n\n\n\nI checked all of these manually and it turns out that the Jeff Brown listed is not the Jeff Brown that was drafted in 1996, so he should be excluded (the Jeff Brown drafted in 1996 played in 0 NHL games). All other players are correct.\n\n\nCode\nold_ps_players |&gt; \n  filter(player != \"Jeff Brown\") |&gt;\n  pivot_longer(cols = c(gp_1997, gp_1998), \n               names_to = \"year\", \n               values_to = \"gp\") |&gt; \n  group_by(year) |&gt; \n  summarize(total_gp = sum(gp), \n            n = length(which(gp &gt; 0))) |&gt; \n  gt()\n\n\n\n\n\n\n\n\nyear\ntotal_gp\nn\n\n\n\n\ngp_1997\n290\n6\n\n\ngp_1998\n1066\n26\n\n\n\n\n\n\n\nWe can see a very small proportion of the players in our dataset (approximately 5400 players) played games under the old PS formula and that these games represent an insignificant proportion of the games in our dataset (there are 774,820 games in total, the old PS games represent about 0.175% of these). Thus we can see that the different PS formulas is not a major concern and is unlikely to significantly impact our results.\nNote also that we will also use GP (games played) as a measure of player success, and that there have been 82 games per regular season for most NHL seasons since 1996 (2004-2005 was cancelled, 2012-2013, 2019-2020, and 2020-2021 were shortened to 48 games, between 68-71 games, and 56 games, respectively). It is unlikely these will meaningfully affect our results given how many games are in our dataset. We can now proceed to the Tidy step.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Import</span>"
    ]
  },
  {
    "objectID": "tidy.html",
    "href": "tidy.html",
    "title": "4  Tidy",
    "section": "",
    "text": "4.1 Setup\nWe install and load the necessary packages, along with functions from prior chapters.\nCode\n# renv::install(\"rvest\")\n# renv::install(\"tidyverse\")\n# renv::install(\"gt\")\n# renv::install(\"reactable\")\n# renv::install(\"DBI\")\n# renv::install(\"duckdb\")\n# renv::install(\"aws.s3\")\n# renv::install(\"paws\")\n# renv::install(\"arrow\")\n\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(gt)\nlibrary(reactable)\nlibrary(DBI)\nlibrary(duckdb)\nlibrary(aws.s3)\nlibrary(paws)\nlibrary(arrow)\n\nsource(\"functions.R\") # load functions defined in prior chapters",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy</span>"
    ]
  },
  {
    "objectID": "tidy.html#introduction",
    "href": "tidy.html#introduction",
    "title": "4  Tidy",
    "section": "4.2 Introduction",
    "text": "4.2 Introduction\nIn the tidy step, we put the data into tidy form and clean it, which will make the data easier to analyze in the later steps. Despite the table from the previous chapter looking fairly clean, further inspection reveals some issues:\n\n\nCode\ngt(import_draft(start_year)[23:30,])\n\n\n\n\n\n\n\n\noverall\nteam\nplayer\nnat\npos\nage\nto\namateur_team\ngp\ng\na\npts\nx\npim\ngp_2\nw\nl\nt_o\nsv_percent\ngaa\nps\n\n\n\n\n23\nPittsburgh Penguins\nCraig Hillier\nCA\nG\n\n\nOttawa 67's (OHL)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n24\nPhoenix Coyotes\nDaniel Briere\nCA\nC\n18\n2015\nDrummondville Voltigeurs (QMJHL)\n973\n307\n389\n696\n-24\n744\n\n\n\n\n\n\n78.5\n\n\n25\nColorado Avalanche\nPeter Ratchuk\nUS\nD\n19\n2001\nShattuck-St. Mary's School (High-MN)\n32\n1\n1\n2\n-2\n10\n\n\n\n\n\n\n0.6\n\n\n26\nDetroit Red Wings\nJesse Wallin\nCA\nD\n18\n2003\nRed Deer Rebels (WHL)\n49\n0\n2\n2\n-5\n34\n\n\n\n\n\n\n0.2\n\n\n\nRound 2\nRound 2\n\n\n\n\n\nNHL Scoring Stats\nNHL Scoring Stats\nNHL Scoring Stats\nNHL Scoring Stats\nNHL Scoring Stats\nNHL Scoring Stats\nNHL Goalie Stats\nNHL Goalie Stats\nNHL Goalie Stats\nNHL Goalie Stats\nNHL Goalie Stats\nNHL Goalie Stats\n\n\n\nOverall\nTeam\nPlayer\nNat.\nPos\nAge\nTo\nAmateur Team\nGP\nG\nA\nPTS\n+/-\nPIM\nGP\nW\nL\nT/O\nSV%\nGAA\nPS\n\n\n27\nBuffalo Sabres\nCory Sarich\nCA\nD\n18\n2014\nSaskatoon Blades (WHL)\n969\n21\n137\n158\n-9\n1089\n\n\n\n\n\n\n36.0\n\n\n28\nPittsburgh Penguins\nPavel Skrbek\nCZ\nD\n18\n2002\nHC Kladno (Czech)\n12\n0\n0\n0\n1\n8\n\n\n\n\n\n\n0.2\n\n\n\n\n\n\n\nSome problems that immediately come up are:\n\nTwo rows get inserted at the end of every round to indicate the round changed.\nThough not visible because of the usage of gt, numbers are being coded as strings (overall, age, to, etc).\n(At least) one player is missing everything except for their pick number, name, team, position, nationality, and amateur team.\nThe +/- column got renamed to x\nThere were two gp columns, one got automatically renamed to gp2 when we used janitor::clean_names()\n\nBy doing a little bit of detective work with some of the other players with missing values elsewhere in the dataset, we notice that players who never played in the NHL have empty strings listed for everything except for the values attributes listed above. We will have to deal with this in the tidy step. Note that Hockey Reference begins listing player’s ages in the 2001 draft, but we aren’t going to use ages for our analysis so we won’t bother coming up for a remedy for the players drafted between 1996 and 2000. The number of picks in the draft has also changed over the years, we will ignore any player drafted after selection 224 (the number of picks in the 2025 NHL Entry Draft). Finally, it would be helpful to remove the columns we don’t care about.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy</span>"
    ]
  },
  {
    "objectID": "tidy.html#cleaning",
    "href": "tidy.html#cleaning",
    "title": "4  Tidy",
    "section": "4.3 Cleaning",
    "text": "4.3 Cleaning\nWe build a function to tidy the data. In particular, we want it to:\n\nRemove the rows added between rounds.\nCorrect the types of each column so we can use numeric columns in calculations.\nChange gp and ps values to 0 for players who never played in the NHL or have a negative ps.\nStandardize position to either be F (forward, any combination of LW, RW, C), D (defenceman), or G (goalie). Note that Kaspars Astashenko has his position listed as “D/W”, a quick search of the NHL website reveals he was a defenceman.\nIf is.na(to), then the player never played in the NHL, so set it to the draft year.\nAdd a year column so we can adjust the stats of players drafted more recently.\nSelect the columns we care about (year, overall, pos, to, ps, and gp) in that order.\nRemove any players selected after 224\\(^{\\text{th}}\\) overall.\nWe don’t care about +/-, so we can ignore the column being renamed since we won’t be using it anyway.\nIt also turns out that for skaters gp2 is empty and for goalies gp and gp2 will have the same value, so this issue can be resolved by simply selecting gp.\nThe 69\\(^{\\text{th}}\\) pick of 2011 and 123\\(^{\\text{rd}}\\) pick of 2002 were forfeited and are listed as blank rows, so these should be removed.\n\nNote that we cannot remove the round separating rows by removing a specified row number since many of the drafts in our dataset have different numbers of picks per round, and some rounds within the same draft have even had a different numbers of picks.\n\n\nCode\ntidy_draft &lt;- function(year){\n  draft_year_table &lt;- import_draft(year) |&gt; \n    filter(overall != \"Overall\" & overall != \"\" & # remove extra rows\n             as.numeric(overall) &lt; 225 & # remove players drafted after pick 224\n             amateur_team != \"()\") |&gt; # remove invalid/forfeited picks \n    type_convert() |&gt; # fix types \n    mutate(\"year\" = year, \"ps\" = pmax(coalesce(ps, 0), 0), \n           \"gp\" = coalesce(gp, 0), \"to\" = coalesce(to, year), \n           \"pos\" = ifelse(str_count(pos, \"G\") == 1, \"G\", \n                          ifelse(str_count(pos, \"D\") == 1, \"D\", \"F\"))) |&gt; \n    select(year, overall, to, pos, ps, gp) # columns we care about\n  draft_year_table\n}\n\nreactable(tidy_draft(1996))\n\n\n\n\n\n\nThis is the form we will use for analysis later.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy</span>"
    ]
  },
  {
    "objectID": "tidy.html#getting-the-data",
    "href": "tidy.html#getting-the-data",
    "title": "4  Tidy",
    "section": "4.4 Getting the Data",
    "text": "4.4 Getting the Data\nWe now load in all of the data and use rbind() to bind the tables together, giving us a single data frame to work with. This also means that our data will be in a mix of long and wide format, since the year and overall columns are formatted like they are in long format but the to, pos, gp, and ps columns are the same as they would be in a wide format. Note that this function takes 2-3 minutes to run (because of the Sys.sleep(5) line, which is necessary to prevent us getting rate limited). We will find a workaround for this shortly so we don’t have to run this function any more than we have to.\n\n\nCode\nall_data &lt;- do.call(rbind, lapply(seq(start_year, end_year, 1),\n                   \\(x) tidy_draft(x)))\n\n\n\n\n[1] 0\n\n\n[1] 0",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy</span>"
    ]
  },
  {
    "objectID": "tidy.html#verification",
    "href": "tidy.html#verification",
    "title": "4  Tidy",
    "section": "4.5 Verification",
    "text": "4.5 Verification\nWe can check that we loaded the data correctly, there should be between 5250 and 5600 rows (the number of picks in a draft has changed over the years in our dataset as we will see, but is always between 210 and 224 so 25 drafts will be between ) and 6 columns:\n\n\nCode\ndim(all_data) # confirm there are 5250-5600 rows and 6 columns\n\n\n[1] 5425    6\n\n\nCode\nlength(unique(all_data$year)) # confirm all 25 years have been included\n\n\n[1] 25\n\n\nCode\ngt(head(all_data, 10))\n\n\n\n\n\n\n\n\nyear\noverall\nto\npos\nps\ngp\n\n\n\n\n1996\n1\n2015\nD\n64.6\n1179\n\n\n1996\n2\n2008\nD\n25.8\n496\n\n\n1996\n3\n2011\nF\n56.6\n822\n\n\n1996\n4\n2000\nF\n0.0\n3\n\n\n1996\n5\n2007\nD\n8.8\n231\n\n\n1996\n6\n2009\nF\n12.5\n627\n\n\n1996\n7\n2007\nF\n9.2\n545\n\n\n1996\n8\n2004\nD\n0.0\n44\n\n\n1996\n9\n2011\nD\n46.9\n917\n\n\n1996\n10\n2004\nD\n2.7\n209\n\n\n\n\n\n\n\nCode\ngt(all_data[23:30,])\n\n\n\n\n\n\n\n\nyear\noverall\nto\npos\nps\ngp\n\n\n\n\n1996\n23\n1996\nG\n0.0\n0\n\n\n1996\n24\n2015\nF\n78.5\n973\n\n\n1996\n25\n2001\nD\n0.6\n32\n\n\n1996\n26\n2003\nD\n0.2\n49\n\n\n1996\n27\n2014\nD\n36.0\n969\n\n\n1996\n28\n2002\nD\n0.2\n12\n\n\n1996\n29\n2009\nF\n0.0\n337\n\n\n1996\n30\n2012\nF\n5.1\n341\n\n\n\n\n\n\n\nThese checks all returned what they should.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy</span>"
    ]
  },
  {
    "objectID": "tidy.html#storing-the-data",
    "href": "tidy.html#storing-the-data",
    "title": "4  Tidy",
    "section": "4.6 Storing the Data",
    "text": "4.6 Storing the Data\nFinally, recall that the import_data function (and thus the tidy_draft function) both take quite a while to run, especially when importing data for a large number of years. To resolve this issue, we will store the data in an AWS S3 bucket so future chapters can get the data from the S3 bucket instead of from Hockey Reference, making this report render significantly faster. We do this by following the instructions given here, which writes all_data to a parquet file and then saves it in our S3 bucket.\n\n\nCode\nSys.setenv(\"AWS_ACCESS_KEY_ID\" = Sys.getenv(\"AWS_ACCESS_KEY_ID\"),\n           \"AWS_SECRET_ACCESS_KEY\" = Sys.getenv(\"AWS_SECRET_ACCESS_KEY\"), \n           \"AWS_DEFAULT_REGION\" = \"us-east-2\")\nbucket = \"trevor-stat468\"\n\ns3write_using(all_data, FUN = write_parquet, \n              bucket = bucket, object = \"all_data.parquet\")\n\n\nIn this and future chapters, we still start by loading in all of the functions from functions.R, which includes all the functions and global constants defined in all chapters, in addition to querying the data from the AWS S3 bucket using duckdb. The functions.R file is included in this project’s GitHub repo, the part of it loads all_data is also given below (here we load it into all_data_test to show that it actually works). I also used this blog post to help me write the below code.\n\n\nCode\ncon &lt;- dbConnect(duckdb())\n\ndbExecute(con, \"INSTALL httpfs;\")\n\n\n[1] 0\n\n\nCode\ndbExecute(con, \"LOAD httpfs;\")\n\n\n[1] 0\n\n\nCode\nall_data_test &lt;- dbGetQuery(con, \"SELECT * \n                            FROM read_parquet('s3://trevor-stat468/all_data.parquet');\")\n\ndim(all_data_test)\n\n\n[1] 5425    6\n\n\nCode\ngt(head(all_data_test, 10))\n\n\n\n\n\n\n\n\nyear\noverall\nto\npos\nps\ngp\n\n\n\n\n1996\n1\n2015\nD\n64.6\n1179\n\n\n1996\n2\n2008\nD\n25.8\n496\n\n\n1996\n3\n2011\nF\n56.6\n822\n\n\n1996\n4\n2000\nF\n0.0\n3\n\n\n1996\n5\n2007\nD\n8.8\n231\n\n\n1996\n6\n2009\nF\n12.5\n627\n\n\n1996\n7\n2007\nF\n9.2\n545\n\n\n1996\n8\n2004\nD\n0.0\n44\n\n\n1996\n9\n2011\nD\n46.9\n917\n\n\n1996\n10\n2004\nD\n2.7\n209\n\n\n\n\n\n\n\nIn future chapters we will edit this data and will put the new version in the same S3 bucket. We proceed to the Visualize chapter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy</span>"
    ]
  },
  {
    "objectID": "visualize.html",
    "href": "visualize.html",
    "title": "5  Visualize",
    "section": "",
    "text": "5.1 Setup\nWe install and load the necessary packages, along with functions from prior chapters.\nCode\n# renv::install(\"gt\")\n# renv::install(\"ggplot2\")\n# renv::install(\"patchwork\")\n\nlibrary(gt)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nsource(\"functions.R\") # load functions defined in prior chapters",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualize</span>"
    ]
  },
  {
    "objectID": "visualize.html#introduction",
    "href": "visualize.html#introduction",
    "title": "5  Visualize",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction\nIn the visualize step, we will perform some EDA (Exploratory Data Analysis) to get a sense of what our data looks like. Specifically, we will see if there are any patterns or trends that may be useful in the Model chapter.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualize</span>"
    ]
  },
  {
    "objectID": "visualize.html#exploratory-data-analysis-eda",
    "href": "visualize.html#exploratory-data-analysis-eda",
    "title": "5  Visualize",
    "section": "5.3 Exploratory Data Analysis (EDA)",
    "text": "5.3 Exploratory Data Analysis (EDA)\n\n5.3.1 Number of Picks\nRecall that over the years the NHL has changed how many picks there are in each round as franchises have been added. A consequence of this is that the number of rounds has also changed, and the number of total picks in a draft has changed several times throughout our dataset. Recall that we removed all picks after #224, but there could be drafts with fewer than 224 total selections. We check for this:\n\n\nCode\nall_data |&gt; \n  group_by(year) |&gt; \n  summarize(num_picks = n()) |&gt; \n  ggplot(aes(x = year, y = num_picks)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nIndeed the drafts after 2005 all have fewer than 224 selections. Note also that 2002 had an invalid pick and 2011 had a forfeited pick, neither is included in this. Some drafts having fewer pick is not a major problem since very late picks aren’t worth very much anyway, but it is worth noting that several picks from #211 onward have a smaller sample size than picks 1-210.\n\n\n5.3.2 PS and GP Values\nBefore doing any further EDA, we will take the five number summary, mean, and standard deviation of both the GP and PS values to get a sense of what they look like. Recall that the five number summary gives the minimum, 25% quantile, median, 75% quantile, and maximum of a dataset. Additionally, recall that PS stands for Point Share, and is a measure of a player’s career contributions to points in the standings (ie the points you get from wins, not the one that is goals plus assists).\n\n\nCode\nc(\"five num\" = fivenum(all_data$gp), \"mean\" = mean(all_data$gp), \"sd\" = sd(all_data$gp))\n\n\nfive num1 five num2 five num3 five num4 five num5      mean        sd \n   0.0000    0.0000    0.0000  145.0000 1779.0000  142.8240  274.0671 \n\n\nCode\nc(\"five_num\" = fivenum(all_data$ps), \"mean\" = mean(all_data$ps), \"sd\" = sd(all_data$ps))\n\n\n five_num1  five_num2  five_num3  five_num4  five_num5       mean         sd \n  0.000000   0.000000   0.000000   3.000000 217.800000   8.252903  21.185903 \n\n\nClearly both the GP and PS values are right skewed. Note that the maximum of the GP data is around 6 standard deviations from the mean \\((\\frac{1779-142.7713}{274.0303} = 5.97)\\) , whereas the maximum of the PS data is almost 10 standard deviations away \\((\\frac{217.8-8.249862}{21.172591} = 9.89)\\).\nWe next check what proportion of our dataset ever played in an NHL game and what proportion generated more than 2 PS in their career, which is the value of exactly one win in the NHL. Based on the research of Luo (2024), we also plan to create a metric based on the proportion of players in our dataset who played in less than 200 NHL games (note that in the Transfom chapter we will use a slightly different metric for active players and goalies, but since we are just exploring the data this will be sufficient for now).\n\n\nCode\nall_data |&gt; \n  filter(gp &gt; 0) |&gt; \n  nrow() / nrow(all_data)\n\n\n[1] 0.4812903\n\n\nCode\nall_data |&gt; \n  filter(ps &gt; 2) |&gt; \n  nrow() / nrow(all_data)\n\n\n[1] 0.2709677\n\n\nCode\nall_data |&gt; \n  filter(gp &gt;= 200) |&gt; \n  nrow() / nrow(all_data)\n\n\n[1] 0.2171429\n\n\nThis tells us that just over half of our dataset never played in an NHL game, almost three quarters made minimal on-ice contributions in their career, and that approximately 20% of our dataset played in at least 200 NHL games.\nTo get visual confirmation that our data is very right skewed, we check the histograms of the data, one of GP (on the left) and one of PS (on the right). We also set the scales to be the same to make comparing the values easier.\n\n\nCode\ngp_hist &lt;- all_data |&gt; \n  ggplot(aes(gp)) + \n  geom_histogram() + \n  scale_y_continuous(limits = c(0, 4200)) + \n  labs(title = \"Distribution of GP\", \n       x = \"GP\", y = \"Number of Players\") \n\nps_hist &lt;- all_data |&gt; \n  ggplot(aes(ps)) + \n  geom_histogram() + \n  scale_y_continuous(limits = c(0, 4200)) + \n  labs(title = \"Distribution of PS\", \n       x = \"PS\", y = \"Number of Players\") \n\ngp_hist + ps_hist\n\n\n\n\n\n\n\n\n\nIndeed, both of these are very right-skewed, and clearly a lot of players end up playing a small number of games and are thus not generating able to generate much PS. The fact that there are more players with a small PS than a small GP also makes sense since players could be unproductive in ~75 games, which would take them out of the first bin for GP while they remain ib the first bin for PS.\n\n\n5.3.3 Picking a Metric\nWe may also guess that GP and PS are positively correlated, since better players get to play in more games and thus accumulate more PS. Indeed:\n\n\nCode\ncor(all_data$ps, all_data$gp)\n\n\n[1] 0.8558304\n\n\nThe relationship is very clear when we split the points by position. The first plot is good because it makes comparing points easier, the other three are nice because they are less crowded.\n\n\nCode\nplot_comb &lt;- ggplot(all_data, aes(x = gp, y = ps, col = pos)) + \n  geom_point(alpha = 0.35)\n\nplot_facet &lt;-  ggplot(all_data, aes(x = gp, y = ps)) + \n  geom_point(alpha = 0.25) + \n  facet_wrap(~pos)\n\nplot_comb / plot_facet\n\n\n\n\n\n\n\n\n\nI suspect this strong correlation (especially for goalies) is largely because players who are good and get lots of PS play for longer and thus play in more GP. Because of this, we choose to only include one of GP and PS in each model to avoid multicollinearity concerns. As mentioned in the Approach section, we will use two GP-related metrics and two PS-related metrics. For each of these metrics we will fit a linear and non-linear regression model, for a total of 8 models. We now explore the GP and PS values in greater detail.\n\n\n5.3.4 GP and PS of Draft Classes\nWe wish to confirm that players selected earlier in a draft (ie a lower overall) tend to play in more games and generate more PS during their careers than those selected later. To check this, we start by creating plot of the GP and PS values by overall for a single draft. Note we will use scales = \"free\" because the scales of GP and PS are quite different:\n\n\nCode\nset.seed(468) # for reproducibility\nrand_year &lt;- sample(start_year:end_year, 1) # year is 2015\n\ngp_plot_1 &lt;- all_data |&gt; \n  filter(year == rand_year) |&gt; \n  ggplot(aes(x = overall, y = gp)) +\n  geom_point() +\n  labs(x = \"Pick Number\", y = \"GP\", \n       title = str_glue(\"GP of Players Drafted in {rand_year}\"), \n       subtitle = \"Note the plots have different scales\")\n\nps_plot_1 &lt;- all_data |&gt; \n  filter(year == rand_year) |&gt; \n  ggplot(aes(x = overall, y = ps)) +\n  geom_point() +\n  labs(x = \"Pick Number\", y = \"PS\", \n       title = str_glue(\"PS of Players Drafted in {rand_year}\"), \n       subtitle = \"Note the plots have different scales\")\n\ngp_plot_1 + ps_plot_1\n\n\n\n\n\n\n\n\n\nSince clearly so many players play in 0 games and thus generate 0 PS, we will recreate the plot without the players that played in 0 NHL games to make the plot easier to read. This time we will include multiple drafts.\n\n\nCode\ngp_plot_2 &lt;- all_data |&gt; \n  filter(gp &gt; 0) |&gt; \n  ggplot(aes(x = overall, y = gp)) + \n  geom_point(alpha = 0.5) + \n  labs(title = \"Distribution of GP\",\n       subtitle = \"Players with ≥ 1 game only; \\nnote the plots have different scales\", \n       x = \"GP\", y = \"Number of Players\")\n\nps_plot_2 &lt;- all_data |&gt; \n  filter(gp &gt; 0) |&gt; \n  ggplot(aes(x = overall, y = ps)) + \n  geom_point(alpha = 0.5) + \n  labs(title = \"Distribution of PS\",\n       subtitle = \"Players with ≥ 1 game only; \\nnote the plots have different scales\", \n       x = \"PS\", y = \"Number of Players\")\n\ngp_plot_2 + ps_plot_2\n\n\n\n\n\n\n\n\n\nThese plots are quite dense and difficult to interpret, but there isn’t really any point in jittering the data because it’ll still overlap, so we re plot them with a random 5 year sample of our dataset.\n\n\nCode\nyears &lt;- sample(start_year:end_year, 5) # years are 2018, 1996, 1999, 2010, 2004\n\ngp_plot_3 &lt;- all_data |&gt; \n  filter(gp &gt; 0 & year %in% years) |&gt; \n  ggplot(aes(x = overall, y = gp)) + \n  geom_point(alpha = 0.5) + \n  labs(title = \"Distribution of GP for 5 Drafts\",\n       subtitle = \"Players with ≥ 1 game only; \\nnote the plots have different scales\", \n       x = \"GP\", y = \"Number of Players\")\n\nps_plot_3 &lt;- all_data |&gt; \n  filter(gp &gt; 0 & year %in% years) |&gt; \n  ggplot(aes(x = overall, y = ps)) + \n  geom_point(alpha = 0.5) + \n  labs(title = \"Distribution of PS for 5 Drafts\",\n       subtitle = \"Players with ≥ 1 game only; \\nnote the plots have different scales\", \n       x = \"PS\", y = \"Number of Players\")\n\ngp_plot_3 + ps_plot_3 \n\n\n\n\n\n\n\n\n\nThough these plot are still quite busy, it shows a strange trend that there seems to be more players drafted around 200 overall that end up having successful careers, than drafted around 125. I am not sure of an underlying reason for this, but we will need to be careful when modeling to ensure that these late picks are not given more value than earlier picks.\nThe plots above are not great because they are missing the vast majority of our dataset (20 drafts plus all the players who played in 0 NHL games for the 5 years in the sample). To improve this, we plot the mean GP and mean PS of the players selected at each pick. We will use the non-aggregated PS and GP values when fitting a model, but the relationship between Overall and PS or GP is more clear when the values are averaged.\n\n\nCode\ngp_plot_4 &lt;- all_data |&gt; \n  group_by(overall) |&gt; \n  summarize(mean_gp = mean(gp)) |&gt; \n  ggplot(aes(x = overall, y = mean_gp)) + \n  geom_point() +\n  geom_point(aes(x = 156, y = mean(filter(all_data, overall==156)$gp)), col = \"dodgerblue\") +\n  geom_point(aes(x = 205, y = mean(filter(all_data, overall==205)$gp)), col = \"salmon\") +\n  labs(title = \"Mean GP by Pick Number\", subtitle = \"Note the scales are different\",\n       x = \"Pick Number\", y = \"Mean GP\") + \n  annotate(geom = \"segment\", x = 156, y = 450, xend = 156, yend = 220, colour = \"dodgerblue\",\n    arrow = arrow(type = \"open\", length = unit(0.32, \"cm\"))) +\n  annotate(geom = \"label\", x = 90, y = 400,\n    label = \"156th overall selection,\\n(mean GP of 203)\",\n    hjust = \"left\", colour = \"dodgerblue\")\n\nps_plot_4 &lt;- all_data |&gt; \n  group_by(overall) |&gt; \n  summarize(mean_ps = mean(ps)) |&gt; \n  ggplot(aes(x = overall, y = mean_ps)) + \n  geom_point() +\n  geom_point(aes(x = 156, y = mean(filter(all_data, overall==156)$ps)), col = \"dodgerblue\") +\n  geom_point(aes(x = 205, y = mean(filter(all_data, overall==205)$ps)), col = \"salmon\") +\n  labs(title = \"Mean PS by Pick Number\", subtitle = \"Note the scales are different\",\n       x = \"Pick Number\", y = \"Mean PS\") + \n  annotate(geom = \"segment\", x = 175, y = 37.5, xend = 203, yend = 14, colour = \"salmon\",\n    arrow = arrow(type = \"open\", length = unit(0.32, \"cm\"))) +\n  annotate(geom = \"label\", x = 100, y = 39,\n    label = \"205th overall selection,\\n(mean PS of 13.124)\",\n    hjust = \"left\", colour = \"salmon\")\n\ngp_plot_4 + ps_plot_4\n\n\n\n\n\n\n\n\n\nInterestingly, points that appear to be outliers in mean GP may not be outliers in mean PS (and vise versa). This is indicated by the blue and pink points in each plot. We can also see that in general GP and PS both tend to decrease later in drafts, and that pick value tends to level off around pick 75, though there are some picks that stick out (for example, pick 205 has an average PS of 13.124, whereas pick 204 has an average PS of 4.856). This particular outlier is due to Henrik Lundqvist and Joe Pavelski being selected at this spot and having career PSs of 173.3 and 130.1, respectively. Very few players drafted this late make it to the NHL (20 of the 25 players in our dataset drafted at pick 205 have more than 0.3 PS), so two players with very successful careers skewing the GP and PS data is not surprising.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualize</span>"
    ]
  },
  {
    "objectID": "visualize.html#summary",
    "href": "visualize.html#summary",
    "title": "5  Visualize",
    "section": "5.4 Summary",
    "text": "5.4 Summary\nWe take the following lessons from our EDA into our Transform and Model chapters:\n\nEach model should only use one of GP or PS to avoid multicollinearity-related issues.\nWe will need to fit a curve or line to the data, since if we simply set \\(v_i\\) to be the mean GP or PS of players selected at pick \\(i\\) then we will fail the first fundamental requirement, that \\(v_i &gt; v_{i+k}\\) for all \\(i, k \\in \\mathbb Z^+\\).\n\nThe GP by pick number and PS by pick number metrics are two of the metrics we will use in the Model chapter.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualize</span>"
    ]
  },
  {
    "objectID": "transform.html",
    "href": "transform.html",
    "title": "6  Transform",
    "section": "",
    "text": "6.1 Setup\nWe install and load the necessary packages, along with functions from prior chapters.\nCode\n# renv::install(\"tidyverse\")\n# renv::install(\"dplyr\")\n# renv::install(\"gt\")\n# renv::install(\"reactable\")\n# renv::install(\"patchwork\")\n# renv::install(\"stringr\")\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(gt)\nlibrary(reactable)\nlibrary(patchwork)\nlibrary(stringr)\n\nsource(\"functions.R\") # load functions defined in prior chapters",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transform</span>"
    ]
  },
  {
    "objectID": "transform.html#introduction",
    "href": "transform.html#introduction",
    "title": "6  Transform",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nAt the end of the Visualize step we looked at the mean GP and mean PS of players selected at pick \\(i\\). These are the first two of the four metrics that will be used to fit a model in the Model chapter. In the Transform chapter, we will create the other two metrics. First, we will implement the method taken by Luo (2024) which is to define a draft pick to be a “success” if that prospect becomes an NHL regular, which is defined to be playing in \\(\\ge\\) 200 NHL games for skaters. We will then look at the success rate of each pick number for the drafts in our dataset. That is, we will set \\(p_i = \\frac{\\text{number of successfull picks at pick $i$}}{\\text{number of $i^{th}$ overall picks}}\\) . Recall in our EDA section we mentioned that some drafts had fewer than 224 selections, which is why we divide by the number of \\(i^{th}\\) overall picks instead of always dividing by 25. Note that Luo was using this method for slightly different analysis (he was evaluating prospect quality to better evaluate prospects, not value draft picks), but the general idea is very similar.\nThe other transformation we will make in this step is to adjust our PS to better represent the careers of actve players (ie those who have not retired yet). Recall that in the Visualize chapter we looked at the average PS of all players in out dataset drafted at pick \\(i\\). As mentioned in the constraints section, one problem with this approach is that players drafted more recently will have had fewer years to generate PS. We can see in the plot below that, unsurprisingly, the total PS is quite a bit lower for the drafts between 2016 and 2020:\n\n\nCode\nall_data |&gt; \n  group_by(year) |&gt; \n  summarize(total_ps = sum(ps)) |&gt;\n  ggplot(aes(x = year, y = total_ps)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nConsidering this, it makes sense to make an adjustment to active players based on an estimate of the PS they will generate in the remainder of their career.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transform</span>"
    ]
  },
  {
    "objectID": "transform.html#transforming-ps",
    "href": "transform.html#transforming-ps",
    "title": "6  Transform",
    "section": "6.3 Transforming PS",
    "text": "6.3 Transforming PS\n\n6.3.1 Introduction\nA similar issue to the one raised above is that there are quite a few current players in our dataset who can still generate additional PS because they are still playing. The issues are related as it is probably reasonable to expect that the discrepancy between the 2016-2020 PS totals and the other years is at least partially due to there being so many active players from those seasons.\n\n\nCode\nall_data |&gt; \n  filter(to == 2025) |&gt; \n  group_by(year) |&gt; \n  summarize(current_players = n()) |&gt;\n  ggplot(aes(x = year, y = current_players)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\nWe can see that our dataset has quite a few active players, and the full value of these players’ careers cannot be fully known. As an aside, it is interesting that PS is so high for the drafts between 2010 and 2015 even though there are still so many active players from those drafts, I did some research but I could not find anything about it. One of my personal theories is that elite players might be skewing things, but Connor McDavid was drafted in 2015 and even he “only” has about 125 PS which is not nearly enough to skew the data on its own. I guess it is possible there were just really strong drafts, but it seems odd that there would be 6 strong drafts in a row.\nEstimating the remaining value of a player’s career could be an entire project all on its own, so we have 3 options:\n\nOnly include drafts from before 2003 (there are no active NHL players who were drafted in 2002 or earlier).\nIgnore this issue altogether (as if every active player in our dataset retired right now).\nMake some sort of adjustment to the PS values of active players to account for the remainder of their career.\n\nNeither of the first two options are particularly appealing. The first is unideal because of sample size concerns and the changes that have taken place in terms of draft eligibility and strategy since the 1980s (which we would need to include to maintain a sample size of 25). Additionally, the second option is arguably worse because it will severely underestimate the quality of star players drafted in the last few years (elite players can play for 15-20 seasons, so we could be missing three quarters of a player’s career if he was drafted in 2020). Thus the only remaining option is to attempt to estimate the remaining value of a player’s career.\nEstimating the remaining value of a player’s career could be an entire project all on its own, but we will make an adjustment that should reduce the impact of this issue (while acknowledging that we will not completely fix it). Let \\(gp_{ij}\\) and \\(ps_{ij}\\) be the GP and PS of the player drafted at pick \\(i\\) of draft \\(j\\). If the player is retired then no adjustment is required, so we set \\(ps_{ij}^{adj} = ps_{j}\\). If that player has not retired, we will set \\(ps^{adj}_{ij} = ps_{ij} + \\frac{ps_{ij}}{gp_{ij}} \\times \\hat{gr_j}\\), where \\(\\hat {gr_j}\\) is our estimate of the number of games left in the career of players drafted in year \\(j\\). We will set \\(\\hat{gr_j} = \\frac{gp_{ij}}{years_{ij}} \\cdot \\hat{yr_j}\\), where \\(years_{ij}\\) is the number of years since player \\(ij\\) was drafted and \\(\\hat{yr_j}\\) is the estimated number of years left in their career. Thus the estimated number of games remaining in a player’s career is their average number of games per season times the estimated number of years remaining. Simplifying, we have \\(ps_{ij}^{adj} = ps_{ij} + \\frac{ps_{ij}}{years_{ij}} \\cdot\\hat{yr_{ij}}\\). We will estimate \\(yr_j\\) later using data from 1996-2004. When we code this adjustment later in this chapter we will verify that the PS values of drafts with a large number of active players end up looking similar to drafts where everyone is retired.\n\n\n6.3.2 Estimating Remaining Career Length\nLet \\(yr_j\\) be the number of years players drafted in draft \\(j\\) have remaining in their career, given they were drafted \\(k\\) years ago. We aim to estimate this value, and will do so by setting \\(\\hat{yr_j}\\) to be the mean career length of players who retired at \\(k\\) years after being drafted and were drafted between 1996 and 2004. We calculate it for \\(1 \\le k \\le 22\\) to make the indexing more intuitive, even though we will only be using the values \\(k \\ge 5\\), since no active player drafted in 2020 or earlier can have played for less than 4 seasons.\n\n\nCode\nget_length &lt;- Vectorize(function(len){ \n  all_data |&gt; \n    mutate(rem_career_len = to - year - len) |&gt; \n    filter(year %in% 1996:2004 & rem_career_len &gt;= 0) |&gt; \n    summarize(mean = mean(rem_career_len)) |&gt; \n    pull(mean)\n})\n\nest_yr &lt;- data.frame(k = seq(1,22)) |&gt;\n  mutate(yr = get_length(k)) \n\nggplot(est_yr, aes(x = k, y = yr)) + \n  geom_point() \n\n\n\n\n\n\n\n\n\nCode\ngt(est_yr)\n\n\n\n\n\n\n\n\nk\nyr\n\n\n\n\n1\n9.477981\n\n\n2\n8.601741\n\n\n3\n7.696370\n\n\n4\n7.004577\n\n\n5\n6.474969\n\n\n6\n6.080107\n\n\n7\n5.716814\n\n\n8\n5.333333\n\n\n9\n4.967093\n\n\n10\n4.720000\n\n\n11\n4.238318\n\n\n12\n3.761155\n\n\n13\n3.382263\n\n\n14\n2.992780\n\n\n15\n2.684444\n\n\n16\n2.212766\n\n\n17\n1.773333\n\n\n18\n1.396396\n\n\n19\n1.214286\n\n\n20\n1.125000\n\n\n21\n1.142857\n\n\n22\n1.000000\n\n\n\n\n\n\n\nThe interpretation for this is that a current NHL player who was drafted 1 year ago has an estimated 9.48 years left in their career, a current NHL player who was drafted 5 years ago has an estimated 6.47 years left in their career, and on.\n\n\n6.3.3 Adjusting PS Values\nNow that we have estimated the remaining number of years for each active NHL player, we can estimate the total PS for their career. Recall that in the introduction of this chapter we said \\(ps_{ij}^{adj} = ps_{ij} + \\frac{ps_{ij}}{years_{ij}} \\cdot\\hat{yr_{ij}}\\).\n\n\nCode\nactive_players &lt;- all_data |&gt; \n  filter(to == 2025) |&gt; \n  mutate(career_len = to - year) |&gt; \n  mutate(adj_ps = ps + round(ps / career_len * get_length(career_len), 2)) |&gt; \n  select(-career_len)\n\ninactive_players &lt;- all_data |&gt; \n  filter(to != 2025) |&gt; \n  mutate(adj_ps = ps)\n\n\nall_data_adj &lt;- rbind(active_players, inactive_players)\nreactable(all_data_adj)\n\n\n\n\n\n\nNote that the adjusted PS values for players who are expected to retire soon may not have changed much, if at all. To say it again, this estimation is not perfect, but it is better than the alternatives (using older data or ignoring the issue). In particular, this method assumes all players will continue to generate PS and play in games at the same rate as they have to this point in their career, and that the number of additional years a player will play for only depends on how many years ago they were drafted.\n\n\n6.3.4 Evaluating our Adjustment\nWe can do some checks to see if these estimates seem reasonable. First, we look at a plot of the PS values before and after the adjustment. We will plot all years for sake of comparison, but recall we only made changes for active players, and thus the drafts between 1996 and 2002 were completely unaffected (and thus the points are on top of each other):\n\n\nCode\nall_data_adj |&gt; \n  group_by(year) |&gt; \n  summarize(total_ps = sum(ps), total_adj_ps = sum(adj_ps)) |&gt;\n  pivot_longer(cols = starts_with(\"total\"), \n               names_to = \"metric\", values_to = \"value\") |&gt; \n  ggplot(aes(x = year, y = value, col = metric)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nIt seems our adjustment was worthwhile since the drafts after 2010 (which are the drafts with a large number of active players) saw their PS values go up substantially.\nNext, we check the magnitude of our changes by calculating the mean and standard deviation of \\(p_{ij}\\) for \\(j \\in \\{ 1996, ..., 2004\\}\\), \\(p_{ij}\\) for \\(j \\in \\{ 2012, ..., 2020 \\}\\), and \\(p_{ij}^{adj}\\) for \\(j \\in \\{ 2012, ..., 2020\\}\\). Note that the intervals are the same size and that the first interval is years where almost all players have retired, and the second interval contains years with more heavily adjusted drafts.\n\n\nCode\nmean_sd &lt;- function(data){\n  c(mean(data), sd(data))\n  }\n  \nmean_sd(filter(all_data_adj, year &lt;= 2004)$ps)\n\n\n[1]  8.806005 23.570904\n\n\nCode\nmean_sd(filter(all_data_adj, year &gt;= 2012)$ps)\n\n\n[1]  6.067517 14.869298\n\n\nCode\nmean_sd(filter(all_data_adj, year &gt;= 2012)$adj_ps)\n\n\n[1]  8.873139 21.217618\n\n\nOur adjustment looks quite good since the mean and standard deviation of the first and third sets seem close, indicating the adjusted PS values for drafts we adjusted “look like” the true PS values. Additionally, the first and second sets look quite different, suggesting that the adjustment we made was necessary.\nAnother way we can check if the adjusted drafts seem similar to the drafts which required little adjustment is by comparing a histogram of the drafts between 1996 and 200 with the drafts between 2016 and 2020. Of course, we’re not expecting them to be perfectly identical because there is a fair amount of variation even between drafts that required little to no adjustment (ex 1999 and 2003).\n\n\nCode\nall_data_adj |&gt; \n  pivot_longer(cols = c(ps,adj_ps), \n               names_to = \"metric\", values_to = \"value\") |&gt; \n  filter((year %in% 1996:2000 & metric == \"ps\") | (year %in% 2016:2020 & metric == \"adj_ps\")) |&gt; \n  ggplot(aes(x = value, fill = metric)) + \n  geom_histogram(position = \"dodge\")\n\n\n\n\n\n\n\n\n\nThese are a bit hard to compare because the tail gets so small so fast, but the values close to 0 look similar enough. We can zoom in on the tail, note that there isn’t much to look at after 100:\n\n\nCode\nall_data_adj |&gt; \n  pivot_longer(cols = c(ps,adj_ps), \n               names_to = \"metric\", values_to = \"value\") |&gt; \n  filter((year %in% 1996:2000 & metric == \"ps\") | (year %in% 2016:2020 & metric == \"adj_ps\")) |&gt; \n  ggplot(aes(x = value, fill = metric)) + \n  geom_histogram(position = \"dodge\") + \n  coord_cartesian(xlim = c(8.5, 100))\n\n\n\n\n\n\n\n\n\nThere seem to be a similar number of observations in each bin, so it seems like our estimations are reasonable. Note again that we are clear restrictions and potential sources of error with this approach, and in the model step we will fit a model using both the raw PS values and the adjusted PS values. Before moving on to the model step, we create a plot of the mean PS and adjusted PS values, which is the same as the last plot from the Visualize chapter, except we will be including the adjusted PS values.\n\n\nCode\nall_data_adj |&gt; \n  pivot_longer(cols = c(ps, adj_ps), names_to = \"metric\", values_to = \"value\") |&gt; \n  group_by(metric, overall) |&gt; \n  summarize(mean_val = mean(value)) |&gt; \n  ggplot(aes(x = overall, y = mean_val, col = metric)) + \n  geom_point(alpha = 0.4)\n\n\n`summarise()` has grouped output by 'metric'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nThe mean Adjusted PS values are all are greater than or equal to the mean unadjusted PS values, which is to be expected what we would expect because \\(ps^{adj}_{ij} \\ge ps_{ij}\\) as we add PS to players who are still active NHL players and do not change the PS values of retired players. This is the third of the four metrics we will use in the Model chapter, though we will use the raw Adjusted PS values rather than the aggregated values.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transform</span>"
    ]
  },
  {
    "objectID": "transform.html#transforming-gp",
    "href": "transform.html#transforming-gp",
    "title": "6  Transform",
    "section": "6.4 Transforming GP",
    "text": "6.4 Transforming GP\n\n6.4.1 Introduction\nIn this chapter we will add a column our dataset indicating whether the player became or is on track to become an NHL regular. We will adopt Luo’s definition of an NHL regular being a skater who plays in or is on track to play in 200 games. This is straightforward a player is retired or has already played in enough games, but is more complex if the player is active and has not yet reached the required number of games. For these players, we will take an almost identical approach as Luo (2024), which changes the GP threshold depending on when a player was drafted. The modified threshold for skaters is given below, where \\(j\\) is the year the player was drafted in \\(t_j\\) is the threshold for players drafted in year \\(j\\).\n\\[t_j = \\begin{cases}200 \\text{,                if $j \\le 2017$}\\\\ \\frac{82\\times(2025-j)}{3} \\text{,    if $j \\in \\{ 2018, 2019, 2020 \\}$}\\\\ \\end{cases}\\]\nLuo did not consider goalies at all in his paper, so one of the changes we will make from Luo’s equation is that the threshold for goalies will be \\(\\frac{t_j}{2}\\) because goalies take longer to develop and even the best goalies only play in approximately 75% of their teams’ games. Thus 100 games for a goalie is around 2.5 seasons’ worth, which is roughly equivalent to 200 games for a skater.\nThe only other difference between our threshold is Luo’s is that the years are different, since Luo’s work is from a year ago and used data from slightly different years. Additionally, Luo did some checks to make sure this estimate is appropriate, it turned out to be quite a good predictor of whether a player will play in 200 games. Here is a plot of the skater threshold for games played based on the skater’s draft year.\n\n\n6.4.2 Adding the Indicator\n\n\nCode\nget_threshold &lt;- function(year){  \n  ifelse(year &lt;= 2017, 200, 82/3*(2025-year)) \n} \n\nthresholds &lt;- data.frame(year = seq(2015,2020)) |&gt;    \n  mutate(t = get_threshold(year))  \n\nggplot(thresholds, aes(x = year, y = t)) +    \n  geom_point() +   \n  scale_x_discrete(limits = seq(2015, 2020))\n\n\n\n\n\n\n\n\n\nCode\ngt(thresholds)\n\n\n\n\n\n\n\n\nyear\nt\n\n\n\n\n2015\n200.0000\n\n\n2016\n200.0000\n\n\n2017\n200.0000\n\n\n2018\n191.3333\n\n\n2019\n164.0000\n\n\n2020\n136.6667\n\n\n\n\n\n\n\nWe make this transformation in R, first for retired players, then for active players, and then use rbind ro combine them.\n\n\nCode\nall_data_ret &lt;- all_data_adj |&gt;    \n  filter(to != 2025) |&gt;   \n  mutate(thresh = ifelse(pos == \"G\", 100, 200),           \n         reg = gp &gt;= thresh) |&gt;    \n  select(-thresh)  \n\nall_data_act &lt;- all_data_adj |&gt;    \n    filter(to == 2025) |&gt;    \n  mutate(thresh = ifelse(pos == \"G\", get_threshold(year) / 2,                           \n                         get_threshold(year)),         \n         reg = gp &gt;= thresh) |&gt;    \n  select(-thresh)  \n\nall_data_adj &lt;- rbind(all_data_ret, all_data_act)\n\n\nWe have now added the column necessary to create a model based on whether an NHL player became an NHL regular. We won’t use the aggregated values to fit a model, but to see if this metric has potential we aggregate the data to get the success rate of every pick number and then plot it to ensure the success rate generally decreases ad the draft goes on.\n\n\nCode\nall_data_adj |&gt;    \n  group_by(overall) |&gt;    \n  summarize(rate = mean(reg)) |&gt;    \n  ggplot(aes(x = overall, y = rate)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nThis plot has a similar shape to the plots at the end of the Visualize chapter. We will use the raw TRUE/FALSE values to fit a model in the Model chapter.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transform</span>"
    ]
  },
  {
    "objectID": "transform.html#combining-everything",
    "href": "transform.html#combining-everything",
    "title": "6  Transform",
    "section": "6.5 Combining Everything",
    "text": "6.5 Combining Everything\nNow that we have the metrics we will use in the Model chapter, we will combine them into a single data frame to make the modelling as straightforward as possible. We also make a plot of the average values by pick each metric, note that no points can be directly on top of each other because all points have different overall values. When we fit the models we will use the raw values, but as seen in the Visualize chapter it is basically impossible to infer anything from the raw value plots. Note that the following two Stack Overflow posts were particularly helpful when writing this code:\n\nDynamic Variable naming in r\nSpecifying column with its index rather than name\n\n\n\nCode\nall_data_comb &lt;- all_data_adj |&gt; \n  group_by(overall) |&gt; \n  summarize(mean_ps = mean(ps), \n            mean_gp = mean(gp), \n            mean_adj_ps = mean(adj_ps),\n            suc_rate = mean(reg))\n\nmetrics &lt;- c(\"mean_ps\", \"mean_gp\", \"mean_adj_ps\", \"suc_rate\")\nnames &lt;- c(\"Mean PS\", \"Mean GP\", \"Mean Adjusted PS\", \"Success Rate\")\n\nfor(i in 1:length(metrics)){\n  assign(str_glue(\"plot_{metrics[i]}\"), \n         ggplot(all_data_comb, aes_string(x = \"overall\", y = metrics[i])) + \n           geom_point() + \n           labs(title = str_glue(\"{names[i]} verses Overall\"), \n                x = \"Overall\", y = str_glue(\"{names[i]}\")))\n}\n\n(plot_mean_ps + plot_mean_gp) / (plot_mean_adj_ps + plot_suc_rate)\n\n\n\n\n\n\n\n\n\nThese plots all have the same general shape, though some of them are on different scale. In the next chapter we will fit a model to the raw data, put the models on the same scale, and evaluate the models.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transform</span>"
    ]
  },
  {
    "objectID": "transform.html#storing-the-transformations",
    "href": "transform.html#storing-the-transformations",
    "title": "6  Transform",
    "section": "6.6 Storing the Transformations",
    "text": "6.6 Storing the Transformations\nAs touched on at the very end of the Tidy chapter, we will be storing our data frames in our S3 bucket. We use the same code to add all_data_adj and all_data_comb to our S3 bucket within AWS. In functions.R we get these data frames by querying the S3 bucket using duckdb.\n\n\nCode\nSys.setenv(\"AWS_ACCESS_KEY_ID\" = Sys.getenv(\"AWS_ACCESS_KEY_ID\"),\n           \"AWS_SECRET_ACCESS_KEY\" = Sys.getenv(\"AWS_SECRET_ACCESS_KEY\"), \n           \"AWS_DEFAULT_REGION\" = \"us-east-2\")\nbucket = \"trevor-stat468\"\n\ns3write_using(all_data_adj, FUN = write_parquet, \n              bucket = bucket, object = \"all_data_adj.parquet\")\ns3write_using(all_data_comb, FUN = write_parquet, \n              bucket = bucket, object = \"all_data_comb.parquet\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transform</span>"
    ]
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "7  Model",
    "section": "",
    "text": "7.1 Setup\nWe install and load the necessary packages, along with functions from prior chapters.\nCode\n# renv::install(\"patchwork\")\n# renv::install(\"stringr\")\n# renv::install(\"reactable\")\n# renv::install(\"pins\")\n# renv::install(\"vetiver\")\n# renv::install(\"plumber\")\n# renv::install(\"aws.s3\")\n\nlibrary(patchwork)\nlibrary(stringr)\nlibrary(reactable)\nlibrary(pins)\nlibrary(vetiver)\nlibrary(plumber)\nlibrary(aws.s3)\n\nsource(\"functions.R\") # load functions defined in prior chapters",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model</span>"
    ]
  },
  {
    "objectID": "model.html#introductionrecap",
    "href": "model.html#introductionrecap",
    "title": "7  Model",
    "section": "7.2 Introduction/Recap",
    "text": "7.2 Introduction/Recap\nNow that we have metrics representing different ways of calculating the historical value of a draft pick, we can now develop models for predicting the value of future picks. First, we will fit a linear regression model to the data, an then we will develop a model via non-linear regression. We will then put the models on the same scale by multiplying each predicted value by a constant, allowing us to compare models more effectively. Note that the following two Stack Overflow posts were once again very helpful when writing the code in this chapter:\n\nDynamic Variable naming in r\nSpecifying column with its index rather than name\n\nRecall the four plots we ended the Transform chapter with this plot, based on the mean PS, mean GP, success rate, and mean adjusted PS for every selection between 1 and 224. For convenience we replot this data below:\n\n\nCode\n(plot_mean_ps + plot_mean_gp) / (plot_mean_adj_ps + plot_suc_rate)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model</span>"
    ]
  },
  {
    "objectID": "model.html#linear-regression",
    "href": "model.html#linear-regression",
    "title": "7  Model",
    "section": "7.3 Linear Regression",
    "text": "7.3 Linear Regression\nWe use lm to fit a linear model to each of the metrics. Note we use a logistic regression model for fitting the model which estimates the probability of a player becoming an NHL regular.\n\n\nCode\nmetrics &lt;- c(\"ps\", \"gp\", \"adj_ps\")\noverall &lt;- all_data_adj$overall\n\nlin_models &lt;- lapply(metrics, \\(x) lm(all_data_adj[[x]] ~ overall))\n\nlogist_model &lt;- glm(all_data_adj$reg ~ overall, family = \"binomial\")\n\n\nFor each linear model, we generate the fitted values. The fitted values are given in the table below:\n\n\nCode\nlm_pred_vals &lt;- lapply(seq(1,3), \n                      \\(x) predict(lin_models[[x]], \n                                   data.frame(overall = seq(1,224))))\n\nlogist_pred_vals &lt;- predict(logist_model, data.frame(overall = seq(1,224)))\n\n\nlm_pred_vals &lt;- data.frame(overall = seq(1, 224), \n                          ps = lm_pred_vals[[1]], \n                          gp = lm_pred_vals[[2]], \n                          adj_ps = lm_pred_vals[[3]], \n                          p_reg = exp(logist_pred_vals) / \n                            (1 + exp(logist_pred_vals)))\n\nreactable(round(lm_pred_vals, 4))\n\n\n\n\n\n\nNext we plot the fitted values. The fitted lines are plotted on top of the average values, we use the average values to make the plot easier to read (but the lines were fit using the raw values).\n\n\nCode\nnames &lt;- c(\"PS\", \"GP\", \"Adjusted PS\", \"Probability of Success\")\n\nfor(i in 1:length(metrics)){\n  assign(str_glue(\"plot_{metrics[i]}\"), \n         ggplot(all_data_comb, aes_string(x = \"overall\", y = str_glue(\"mean_{metrics[i]}\"))) + \n           geom_point(alpha = 0.5) + \n           labs(title = str_glue(\"{names[i]} verses Overall\"), \n                x = \"Overall\", y = str_glue(\"{names[i]}\")))\n}\n\nfor(i in 1:length(metrics)){\n  assign(str_glue(\"plot_lm_{metrics[i]}\"), \n         get(str_glue(\"plot_{metrics[i]}\")) + \n           geom_line(data = lm_pred_vals, aes_string(x = \"overall\", y = metrics[i]), \n                     col = \"red\", lwd = 1.5))\n}\n\nplot_lm_p_reg = ggplot(all_data_comb, aes(x = overall, y = suc_rate)) + \n  geom_point(position = \"jitter\", alpha = 0.5) + \n  geom_line(data = lm_pred_vals, aes(x = overall, y = p_reg), col = \"red\", lwd = 1.5)\n\n(plot_lm_ps + plot_lm_gp) / (plot_lm_adj_ps + plot_lm_p_reg)\n\n\n\n\n\n\n\n\n\nBased on the plots above, all four of these linear models are inadequate. Moreover, all of the models except for the one based on NHL regular probability fail our second requirement for a feasible model, which is that all picks have a strictly positive value. With this in mind, we move onto fitting a non-linear model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model</span>"
    ]
  },
  {
    "objectID": "model.html#non-linear-regression",
    "href": "model.html#non-linear-regression",
    "title": "7  Model",
    "section": "7.4 Non-Linear Regression",
    "text": "7.4 Non-Linear Regression\nGiven that none of the four linear models were appropriate, we will reattempt to fit a model using non-linear regression (ie the nls function, which stands for non-linear least squares). The resource Non-linear Regression in R was very helpful when working on this section. In short, we will be fitting the model\n\\[ v_{i,m} = \\frac{\\phi_{1, m}}{1+ (\\frac{e^{\\phi_{2,m}}}{i})^{1/\\phi_{3,m}}} \\]\nWhere\n\n\\(i\\) is the pick number.\n\\(m\\) is the metric being used.\n\\(v_{i,m}\\) is the value of pick \\(i\\) based on metric \\(m\\).\n\\(\\phi_{1, m},\\phi_{2, m},\\phi_{3, m}\\) are parameters we are estimating which depend on which metric we are using.\n\nWe choose to use nls because it allows us to directly fit a model with non-linear parameters, we do not need to transform the explanatory or response variates so it makes interpretations significantly easier. We fit the models using the same metrics as before except for the model based on whether players become NHL regulars.\n\n\nCode\nmetrics &lt;- c(\"ps\", \"gp\", \"adj_ps\")\n\noverall &lt;- all_data_adj$overall\n\nfor(i in seq(1,3)){\n  assign(str_glue(\"nls_{metrics[i]}\"), \n         nls(all_data_adj[[i+4]] ~ SSlogis(log(overall), phi1, phi2, phi3)))\n}\n\nnls_pred_vals &lt;- lapply(seq(1,3), \n                        \\(i) predict(get(str_glue(\"nls_{metrics[i]}\")), \n                                   data.frame(overall = seq(1,224))))\nnls_pred_vals &lt;- data.frame(overall = seq(1, 224), \n                          ps = nls_pred_vals[[1]], \n                          gp = nls_pred_vals[[2]], \n                          adj_ps = nls_pred_vals[[3]])\nreactable(round(nls_pred_vals, 4))\n\n\n\n\n\n\nWe now plot the fitted line. We once again use plot the line on top of the historical averages to make the plot easier to read.\n\n\nCode\nfor(i in seq(1,3)){\n  assign(str_glue(\"plot_nls_{metrics[i]}\"), \n         ggplot(all_data_comb, \n                aes_string(x = \"overall\", y = str_glue(\"mean_{metrics[i]}\"))) + \n           geom_point() + \n           geom_line(data = nls_pred_vals, \n                     aes_string(x = \"overall\", y = str_glue(\"{metrics[i]}\")), \n                     col = \"red\", lwd = 1.5))\n}\n\nplot_nls_ps / plot_nls_gp / plot_nls_adj_ps\n\n\n\n\n\n\n\n\n\nNow that we have fit the models, we plot the residual vs overall values. Note that typically we’d plot the residual vs fitted values, but this plot is impossible to make any inferences from because so many of the fitted values are relatively small, meaning they all get clumped together.\n\n\nCode\nall_resids &lt;- data.frame(overall = all_data_adj$overall, \n                         ps_resid = all_data_adj$ps - predict(nls_ps, overall) , \n                         gp_resid =  all_data_adj$gp - predict(nls_gp, overall), \n                         adj_ps_resid = all_data_adj$adj_ps - predict(nls_adj_ps, overall))\n\nfor(i in seq(1,3)){\n  assign(str_glue(\"plot_resid_{metrics[i]}\"), \n         ggplot(all_resids, aes_string(x = \"overall\", y = str_glue(\"{metrics[i]}_resid\"))) + \n           geom_point(alpha = 0.3))\n}\n\nplot_resid_ps / plot_resid_gp / plot_resid_adj_ps\n\n\n\n\n\n\n\n\n\nThis clearly fails a number of the model assumptionts, most notably the assumption regarding a constant residual variance. This is not really that surprising and is not a major cause for concern as pretty much any feasible model will show the same general pattern for a few reasons. First, there is no upper bound on our residuals since (in theory) players can play in infinitely many games or generate infinite PS. However, there is a lower bound on the residuals because if a player never plays in an NHL game and/or never generates any PS, then the associated residual will be the predicted value. That is, there is a limit to how much a player can underperform relative to draft position (since we don’t allow negative PS and negative GP is not possible), but there is no limit to how much they can overperform. One other thing to note is that, generally speaking, there seems to be more variance among earlier picks than later picks because earlier picks can over or underperform a lot, while later picks can overperform a lot or underperform a little (because their expectations are lower, even if they do nothing they didn’t underperform that much).\nWe will look at a QQ Plot of the residuals. This ggplot2 documentation page was very helpful for this part.\n\n\nCode\nfor(i in seq(1,3)){\n  assign(str_glue(\"qq_{metrics[i]}\"), \n         ggplot(all_resids, aes_string(sample = str_glue(\"{metrics[i]}_resid\"))) + \n           stat_qq() + \n           stat_qq_line(col = \"red\", lwd = 0.5))\n}\n\nqq_ps / qq_gp / qq_adj_ps\n\n\n\n\n\n\n\n\n\nClearly the residuals are not normally distributed. This is another result of the points mentioned above, that there is a lower bound on a player’s underperformance, but no upper bound on how much they can overperform.\nThe lesson to take from these plots is that deriving confidence intervals or calculating p-values is almost certainly a bad idea because the assumptions that those mechanisms rely on are clearly invalid. On the other hand, taking point estimates is probably okay since we effectively found a line of best fit which doesn’t rely on any of the model assumptions. With this in mind, we will be relying on the point estimates given by this model for the remainder of this report.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model</span>"
    ]
  },
  {
    "objectID": "model.html#model-selection",
    "href": "model.html#model-selection",
    "title": "7  Model",
    "section": "7.5 Model Selection",
    "text": "7.5 Model Selection\nFor convenience we replot the fitted values from the non-linear models we fit in the last section.\n\n\nCode\nplot_nls_ps + plot_nls_gp + plot_nls_adj_ps\n\n\n\n\n\n\n\n\n\nOne problem with these plots is that they’re all on different scales, which makes comparing models very difficult. Recall from the Introduction chapter that we want to end up with a model which has \\(\\hat v_1 = 1000\\) “points” to maintain consistency with existing work. To do this, we set \\(C_m = \\frac{1000}{v_{1,m}}\\), and then multiply all of the other \\(\\hat v_{i,m}\\) values by \\(C_m\\) for \\(i \\not= 1\\), and then use reactable to make sure it worked.\n\n\nCode\nC_m &lt;- c(1, 1000 / nls_pred_vals[1,-1])\n\nscaled_vals &lt;- nls_pred_vals * C_m\n\nreactable(round(scaled_vals, 4))\n\n\n\n\n\n\nThis seems to look good. Now that the predicted values are on the same scale, we can plot them on top of each other.\n\n\nCode\nggplot(scaled_vals, aes(x = overall)) + \n  geom_line(aes(y = ps), col = \"blue\", lwd = 1.2) + \n  geom_line(aes(y = gp), col = \"limegreen\", lwd = 0.75) + \n  geom_line(aes(y = adj_ps), col = \"salmon\", lwd = 0.8, lty = 4)\n\n\n\n\n\n\n\n\n\nInterestingly, the PS and Adjusted PS lines are basically directly on top of each other, which implies that the adjustment we made had little impact on the predictions. We can also compare the residual sum of squares without rescaling all the values and refitting the models by simply multiplying the RSS values by the appropriate \\(C_m\\): Why doesn’t this work???\n\\[RSS_m  = \\sum^n_{i=1}(\\hat v_{i,m} - v_i)^2 \\Longrightarrow RSS \\times C_m^2 = \\sum^n_{i=1}(C_m\\hat v_{i,m} - C_mv_i)^2 \\]\n\n\nCode\nsummary(nls_ps)\n\n\n\nFormula: all_data_adj[[i + 4]] ~ SSlogis(log(overall), phi1, phi2, phi3)\n\nParameters:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nphi1 162.40720   20.72857   7.835  5.6e-15 ***\nphi2   0.44022    0.28595   1.540    0.124    \nphi3  -1.21381    0.05159 -23.527  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.05 on 5422 degrees of freedom\n\nNumber of iterations to convergence: 2 \nAchieved convergence tolerance: 1.614e-06\n\n\nCode\nsummary(nls_gp)\n\n\n\nFormula: all_data_adj[[i + 4]] ~ SSlogis(log(overall), phi1, phi2, phi3)\n\nParameters:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nphi1 983.47691   53.06846   18.53   &lt;2e-16 ***\nphi2   2.30269    0.13060   17.63   &lt;2e-16 ***\nphi3  -1.02498    0.04346  -23.59   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 233.1 on 5422 degrees of freedom\n\nNumber of iterations to convergence: 3 \nAchieved convergence tolerance: 7.857e-07\n\n\nCode\nsummary(nls_adj_ps)\n\n\n\nFormula: all_data_adj[[i + 4]] ~ SSlogis(log(overall), phi1, phi2, phi3)\n\nParameters:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nphi1 179.18880   19.86941   9.018   &lt;2e-16 ***\nphi2   0.56971    0.24867   2.291    0.022 *  \nphi3  -1.18730    0.04728 -25.111   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.83 on 5422 degrees of freedom\n\nNumber of iterations to convergence: 2 \nAchieved convergence tolerance: 6.537e-07\n\n\nCode\nC_m\n\n\n[[1]]\n[1] 1\n\n$ps\n[1] 10.44172\n\n$gp\n[1] 1.124339\n\n$adj_ps\n[1] 9.034525\n\n\nSo the scaled sum of squares are:\n\n\nCode\nRSS &lt;- c(18.05, 233.1, 19.83)\nRSS * unlist(C_m[-1])\n\n\n      ps       gp   adj_ps \n188.4730 262.0834 179.1546 \n\n\nRecall that since the correlation between PS and GP was \\(\\approx\\) 0.85, we will not use a model which incorporates both due to multicollinearity concerns. Given the choice, we prefer to use a metric based on PS rather than GP for a few reasons.\n\nPS credits players for contributing to their team, whereas GP gives credit for being good enough to play for a team.\nThe RSS associated with the model with GP is significantly higher than the RSS for both of the PS-related models.\nWhile both metrics are right skewed, in this context we prefer a metric which has a longer right tail since this will allow us to distinguish good players from elite players. Specifically, there is a hard cap on how many games a player can play in a certain time frame, but the limit on PS is impossible to reach (a player would have to win every game in his career and be fully responsible for each and every win). In other words, if two players each played in 82 games per season for 10 seasons before retiring, they would both have played in 820 games, but their PS values could be quite different, indicating that PS is a more distinguishing metric. We know PS has a longer tail because the maximum of PS is more standard deviations away from the mean than the maximum of GP, as we showed at the start of our EDA in the Visualize chapter.\nThe PS formula includes time on ice, which tends to be a better measure of player involvement than GP. For example, Player A who plays 20 minutes a night and and Player B who plays 10 minutes a night may have the same GP, but Player A would likely be considered more valuable because he plays twice as much.\n\nNow that we’ve settled either using PS or Adjusted PS, we take a closer look at the difference between their predicted values. The mean disparity between the two predicted values is about 1.04, which indicates the metric we choose is unlikely to significantly impact our conclusion since picks are values out of 1000. Note that we need to take the absoute value since the since the scaled version of Adjusted PS may be smaller than the scaled PS values.\n\n\nCode\nmean(abs(scaled_vals$adj_ps - scaled_vals$ps))\n\n\n[1] 1.044389\n\n\nWe can also plot the percent differences.\n\n\nCode\nggplot(scaled_vals, aes(x = overall)) + \n  geom_line(aes(y = ps / adj_ps), col = \"dodgerblue\") + \n  geom_line(aes(y = adj_ps / ps), col = \"salmon\")\n\n\n\n\n\n\n\n\n\nWe see that most of the variation between the PS and Adjusted PS values occurs late in the draft, but these picks are very close in raw point values since 3% of a relatively small number is a small number. With this in mind, we choose the model based on PS (not Adjusted PS). As the analysis in this section has showed, there is almost zero difference between the models based on PS and Adjusted PS models. Therefore we prefer to use the simpler model, which in this case is PS. The RSS values for both models are fairly close, and even though the RSS is smaller for Adjusted PS we will chose the PS model due to it being a simpler model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model</span>"
    ]
  },
  {
    "objectID": "model.html#finishing-touches",
    "href": "model.html#finishing-touches",
    "title": "7  Model",
    "section": "7.6 Finishing Touches",
    "text": "7.6 Finishing Touches\nTo make the rest of this report and shiny app simpler, we will refit the model with scaled PS values:\n\n\nCode\nscal_ps &lt;- all_data_adj$ps * C_m[[2]]\n\nnls_scal_ps &lt;- nls(scal_ps ~ SSlogis(log(overall), phi1, phi2, phi3))\n\nsummary(nls_scal_ps)\n\n\n\nFormula: scal_ps ~ SSlogis(log(overall), phi1, phi2, phi3)\n\nParameters:\n       Estimate Std. Error t value Pr(&gt;|t|)    \nphi1 1695.81037  216.44187   7.835  5.6e-15 ***\nphi2    0.44022    0.28595   1.540    0.124    \nphi3   -1.21381    0.05159 -23.527  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 188.5 on 5422 degrees of freedom\n\nNumber of iterations to convergence: 2 \nAchieved convergence tolerance: 1.614e-06\n\n\nWe now create two functions which will be used in our R Shiny app. The first is value, which takes in a pick and returns the number of points (a more user-friendly version of predict), and the second is pick, which takes in a number of point and returns the closest pick to that number of points. Recall we fit the model \\[ v_{i,m} = \\frac{\\phi_{1, m}}{1+ (\\frac{e^{\\phi_{2,m}}}{i})^{1/\\phi_{3,m}}} \\]and we found \\(\\phi_{1,m} = 1695.81038, \\phi_{2,m} = 0.4402, \\phi_{3,m} = -1.21381\\). Creating the value function is now straightforward.\n\n\nCode\nphis &lt;- unname(coef(nls_scal_ps))\nphi_1 &lt;- phis[1]\nphi_2 &lt;- phis[2]\nphi_3 &lt;- phis[3]\n\nvalue &lt;- function(overall){\n  phi_1 / (1 + (exp(phi_2) / overall)^(1 / phi_3))\n}\n\n# check it worked:\nvalue(1) # should be 1000 \n\n\n[1] 1000\n\n\nCode\nvalue(224) # should be approx 27.762\n\n\n[1] 27.7623\n\n\nThe pick function is more complex, we need to take the inverse of the value function. It’s not a difficult computation and it’s a few steps so we omit the steps, it turns out the inverse is given below:\n\\[\ni = \\frac{e^{\\phi_{2,m}}}{(\\frac{\\phi_{1,m}}{v_{i,m}} - 1)^{\\phi_{3,m}}}\n\\]\n\n\nCode\npick &lt;- function(value){\n  round(exp(phi_2) / ((phi_1 / value - 1)^phi_3))\n}\n\n# check it worked:\npick(1000) # should be 1 \n\n\n[1] 1\n\n\nCode\npick(27.76) # should be 224\n\n\n[1] 224\n\n\nCode\ni &lt;- seq(1, 224)\nall(pick(value(i)) == i)\n\n\n[1] TRUE\n\n\nNote that the value and pick functions we implemented in R are not perfect inverses of each other because of the use of round, but for the purposes of our R Shiny app this will not be an issue. As one final check, we see what the difference in values was for a recent NHL trade in which the Philadelpha Flyers acquired the 12th overall selection from the Pittsburgh Penguins in exchange for the 22nd and 31st selections. We can see that this difference is about 39 points, which is equivalent to Pittsburgh receiving a mid 5th round pick in surplus value. This seems about right, it is widely accepted that the team that acquires the highest pick almost always gives up more than they receive. This is why teams only trade up if they really like a player who is available at the current pick, but that the team thinks will not be available at their next pick.\n\n\nCode\nvalue(12) - value(22) - value(31)\n\n\n[1] -38.92649\n\n\nCode\npick(38.9271)\n\n\n[1] 147",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model</span>"
    ]
  },
  {
    "objectID": "model.html#storing-the-model",
    "href": "model.html#storing-the-model",
    "title": "7  Model",
    "section": "7.7 Storing the Model",
    "text": "7.7 Storing the Model\nWe now store the model so we can access it from within our Shiny app. Note that some of the steps given on DevOps for Data Science were in Python, I found the equivalent R steps here.\n\n\nCode\n# I couldn't find any info online about the error I was getting so I used \n#   ChatGPT, it turns out vetiver does not work with nls objects, so for\n#   now I'm using one of the lms \nlm_test &lt;- lm(ps ~ overall, data = all_data_adj)\n\nSys.setenv(\"AWS_ACCESS_KEY_ID\" = Sys.getenv(\"AWS_ACCESS_KEY_ID\"),\n           \"AWS_SECRET_ACCESS_KEY\" = Sys.getenv(\"AWS_SECRET_ACCESS_KEY\"), \n           \"AWS_DEFAULT_REGION\" = \"us-east-2\")\nbucket = \"trevor-stat468\"\n\nboard &lt;- board_s3(bucket = bucket) \n\ns3write_using(as.data.frame(scal_ps), FUN = write_parquet, \n              bucket = bucket, object = \"scal_ps.parquet\")\n\n\nv &lt;- vetiver_model(lm_test, \"lm_test\")\nvetiver_pin_write(board, v)\n\n\nCreating new version '20250802T201654Z-c4f2b'\nWriting to pin 'lm_test'\n\nCreate a Model Card for your published model\n• Model Cards provide a framework for transparent, responsible reporting\n• Use the vetiver `.Rmd` template as a place to start\n\n\nWe can also interface with the model using vetiver_api. Note I have set eval = FALSE for this chunk because the report won’t render otherwise, I’ve included a screenshot of the popup instead.\n\n\nCode\npr() |&gt;\n  vetiver_api(v) |&gt; \n  pr_run(port = 8080)\n\n\nHere is a screenshot of the popup that opens when the above code is run. We now proceed to the Communication chapter, which contains the key results from this report.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model</span>"
    ]
  },
  {
    "objectID": "communicate.html",
    "href": "communicate.html",
    "title": "8  Communicate",
    "section": "",
    "text": "This is a placeholder for my communicate chapter and will eventually have plots, tables, etc.\nThis chapter will contain my table, graph, value + pick functions, and instructions for and a link to my shiny app. In my RShiny app, I plan to allow users to enter potential trades, click “evaluate”, and then see the point gain/loss. Additionally, I’d like to put the point difference into context “this is equivalent to team A giving up the 174th overall pick”.\nOther things I need to do:\n\nRead thru instructions to see if I’m missing anything\nGive all plots titles and labels\nRemove unnecessary library calls\nMake sure everything flows logically\nMake notation consistent\nmake sure links work\nMake code more efficient\nSpell check\nAdd references to .bib file\nremove unnecessary files from github (ie holding_file, model2, etc).\n\n3 team trades are incredibly rare in the NHL so they are not directly included in the app, though they could be analyzed by looking at what each team individually receives and gives up (ie look at it in terms of 3 two team trades)\nThis is all the code that I think might be useful but that I currently don’t know what it is for.\n\n\nCode\n# I couldn't find any info online about the error I was getting so I used \n#   ChatGPT, it turns out vetiver does not work with nls objects, so for\n#   now I'm using one of the linear models\n\n# v &lt;- vetiver_model(nls_scal_ps, \"ps_value_model\")\n\nv &lt;- vetiver_model(lin_models[[1]], \"lm_test\")\n\nmodel_board &lt;- board_folder(\"/data/model\")\nmodel_board |&gt;\n  vetiver_pin_write(v)\n\nmodel_board |&gt; \n  vetiver_pin_read(\"lm_test\")\n\n\n\n\ntest_lm &lt;- lm(ps ~ overall, data = all_data_adj)\nv &lt;- vetiver_model(test_lm, \"test_lm\")\n\n\n\n\nmodel_board &lt;- board_temp(versioned = TRUE)\nmodel_board |&gt; \n  vetiver_pin_write(v)\n\n\n\npr() |&gt;\n  vetiver_api(v) |&gt; \n  pr_run(port = 8080)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Communicate</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "ChatGPT Usage (will add link when done)\nValuation of NHL Draft Picks using Functional Data Analysis\nImproving NHL Draft Outcome Predictions using Scouting Reports\nExamining the value of NHL Draft picks\nNHL draft: What does it cost to trade up?\nHockeyReference (for the data used in the report)\nR for Data Science (2e) (throughout the report)\nDevOps for Data Science (throughout the report)\nHappy Git and GitHub for the useR (throughout the report)\nMastering Shiny (for the RShiny app)\nNon-linear Regression in R (in the Model chapter)\nNHL Draft Eligibility (in the Import chapter)\nCalculating Point Shares (throughout the report)\nDynamic Variable naming in r (in the Transform chapter)\nSpecifying column with its index rather than name (in the Transform chapter)\nGitHub Pages\n\nFor now this is just a list of links. Later I will formalize it.",
    "crumbs": [
      "References"
    ]
  }
]