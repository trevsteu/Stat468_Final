[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat468 Final Project",
    "section": "",
    "text": "1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Index</span>"
    ]
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Stat468 Final Project",
    "section": "1.1 Abstract",
    "text": "1.1 Abstract\nIn the days of and leading up to the 2025 NHL Entry Draft there were a total of 18 trades which only included draft picks. This report aims to use player contribution data to determine the relative value of selections in the NHL Entry Draft. To do this, data will be imported from Hockey Reference, and several potential models will be fit to the data with the goal of predicting the value of each pick based on historical outcomes. The RShiny app component of this project allows users to interactively check the fairness of potential trades.\nKnowing the relative value of picks allows NHL teams to both determine whether they should accept trade offers they have received as well as propose favourable trades to other teams. That is, this project makes contributions to both asset valuation as well as trade analysis, both in the context of selections in the NHL Entry Draft.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Index</span>"
    ]
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Stat468 Final Project",
    "section": "1.2 Data",
    "text": "1.2 Data\nThe data used by this report is imported from Hockey Reference, which has data on the NHL Draft and player games played and point share counts dating back to 1963. Each row on Hockey Reference is one player selected, and the columns included on the site are:\n\nOverall: the selection mumber where the player was selected.\nTeam: the team that selected the player.\nName, Nat, Pos, Age: the player’s name, nationality, position, at age at the time of the draft.\nTo: the last year a player played in the NHL. For players who never played in the NHL this will be the empty string, for those who are still playing it will be 2025.\nAmateur Team: the team the player was drafted from (confusingly this could be a pro European team).\nGP, G, A, PTS, +/-, PIM: the player’s career games played, goals, assists, points (goals plus assists), plus minus, and penalty minutes. For players who never played in the NHL this will be empty strings. For goalies, this GP column will match the next GP column for goalies, and values are not necessarily the empty string for goalies (eg a goalie could have gotten an assist). Note that I will abbreviate games played to GP in this report.\nGP, W, L, T/O, SV%, GAA: the goalie’s career games played, wins, losses, ties plus overtime losses, save percentage, and goals against average. For goalies who never played in the NHL and all skaters, these columns will all be empty strings.\nPS: the player’s estimated point shares, or points added to their team over the course of their career (here we mean points in the standings, NOT goals and assists). There is more info on point shares here. Note that I abbreviate point share to PS in this report.\n\nAll stats listed are for regular season games only, as we do not want to put players who played on bad teams at more of a disadvantage. Note that we will only use a subset of the years between 1963 and 2025 and of the attributes listed above, as will be explained in the Import and Tidy chapters.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Index</span>"
    ]
  },
  {
    "objectID": "index.html#constraints",
    "href": "index.html#constraints",
    "title": "Stat468 Final Project",
    "section": "1.3 Constraints",
    "text": "1.3 Constraints\nThere are a number of technical and practical constraints at play. Here are some of them:\n\nThere have only been 63 drafts in NHL history, but drafts which occurred too long ago are likely not relevant and drafts which occurred too recently are difficult to evaluate. This will be discussed further in the Import chapter.\nPlayers drafted earlier in a draft (ie with a better pick) typically get more opportunities than players selected in the later rounds. In particular, teams often fall victim to the sunk cost fallacy because scouts and management look bad when players who they invested a high pick into don’t contribute to the team. We will briefly touch on this in the Visualize chapter. Accounting for this is vert difficult or even impossible, and we will not attempt to remedy it.\nEstimating the value of a player’s career is not a trivial task. We will use point share which is a calculated by Hockey Reference and includes several stats, but it is still not a perfect metric as it can still be dependent on external factors, such as the quality of the player’s team the opportunities the player was given.\nWe are interested in using historical data to predict the value of a draft pick from the team’s perspective. One slight problem with this is that the value of the pick from the team’s perspective depends on how long the player stayed on their team and what (if anything) the team got when the player left the team (via trade, free agency, or retirement). We will ignore this because it is nearly impossible to take these factors into account.\nEvery draft has strong portions and weak portions. For example, one draft might have a very strong second round (by that we mean the prospects drafted in the second round in that particular draft are of higher quality than those typically drafted in the second round). Though this seems like an obvious point, it is crucial to mention because it is a significant asterisk on this report, which will assume all drafts have equal value structures (ie the quality of a prospect #27 overall of draft A is the same as the quality of a prospect drafted at # 27 of draft B).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Index</span>"
    ]
  },
  {
    "objectID": "question.html",
    "href": "question.html",
    "title": "2  Question",
    "section": "",
    "text": "2.1 Approach\nDepending on the context, there can be countless approaches one could take to estimate \\(v_i\\), the value of the \\(i^{\\text{th}}\\) selection of an NHL Entry Draft. As one example, one could estimate \\(v_i\\) by comparing the assets given up and acquired to previous trades and obtain the “market value” of each selection. In contrast, this report will take the approach of defining the value of selection \\(i\\) to be the career contribution of the player drafted at \\(i^{\\text{th}}\\) overall in the upcoming draft. This report will estimate value of pick \\(i\\) (ie find \\(\\hat v_i\\)) by fitting several models incorperating the PSs by the players previously selected at pick \\(i\\) (and possibly selections “close” to \\(n\\)). We will consider two different methods of expressing our data, and for each we will fit a linear regression model, weighted \\(k\\)-nearest neighbour algorithm, and a non-linear regression model. To maintain some consistency with previous related work, this report will define \\(v_i\\) to be “true” value of the \\(i^{\\text{th}}\\) selection, and this value will be in terms of unitless “points”, which are aimed at making comparing the value of picks easier. That is, instead of saying \\(v_i = \\frac{a}{b} v_1\\) for \\(i &gt; 1\\) where \\(a &lt; b\\), we will say \\(v_1 \\approx 1000\\) points, $v_i = c$ points for \\(1000 &lt; c &lt; 0\\).\nThere are critical points that must be mentioned in relation to the models that we will fit. First, clearly we must have that \\(v_i &gt; v_{i+k}\\) for all \\(k \\in \\mathbb Z^+\\), so that later draft picks are not considered more valuable than picks earlier in the draft. This should intuitively make sense because the players available at pick \\(i+k\\) are a proper subset of the players available at pick \\(i\\), so there is no reason for a later pick to be more valuable in a trade context than an earlier pick. Thus for a model to have any chance of being accurate, it must satisfy \\(v_i &gt; v_{i+k}\\) for all \\(k \\in \\mathbb Z^+\\). The second important note is that it is common knowledge in ice hockey circles and confirmed by the previous work listed below is that NHL draft picks do not decrease in value linearly. In particular, \\(v_i\\) decreases quickly in \\(i\\), so the difference in value pick 1 and 30 is much greater than between pick 101 and 130 (ie \\(v_1 - v_{30} &gt;&gt;&gt; v_{101} - v_{130}\\)).\nNote that if picks did decrease linearly in value linearly then it would be very easy to create a model of draft pick value since we would have\n\\[ v_1 = v_2 + c = v_3 + 2c = ... = v_{224} + 223c \\]where \\(c &gt; 0\\), meaning we would only have to find the value of \\(c\\). Note that in the model step we will fit this model and perform a statistical test to show that this model is not appropriate, before quickly moving on to non-linear models.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Question</span>"
    ]
  },
  {
    "objectID": "question.html#previous-work",
    "href": "question.html#previous-work",
    "title": "2  Question",
    "section": "2.2 Previous Work",
    "text": "2.2 Previous Work\nSome work in this area has been done before, such as:\n\nValuation of NHL Draft Picks using Functional Data Analysis\nExamining the value of NHL Draft picks\nNHL draft: What does it cost to trade up?\n\nThis report will most closely follow the work done in the first paper listed. As an interesting aside, Eric Tulsky, who wrote the last article listed above in 2013, was hired as General Manager of the Carolina Hurricanes in 2024.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Question</span>"
    ]
  },
  {
    "objectID": "import.html",
    "href": "import.html",
    "title": "3  Import",
    "section": "",
    "text": "3.1 Setup\nWe install and load the necessary packages.\nCode\n# install.packages(\"rvest\")\n# install.packages(\"stringr\")\n# install.packages(\"tidyverse\")\n# install.packages(\"janitor\")\n# install.packages(\"gt\")\nlibrary(rvest)\nlibrary(stringr)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(gt)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Import</span>"
    ]
  },
  {
    "objectID": "import.html#introduction",
    "href": "import.html#introduction",
    "title": "3  Import",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\nIn the import step we import the data required for this report. As mentioned before, we will be importing data from Hockey Reference. Before we import any data, it’s important to consider which and how many years we want to include in this analysis. Since the NHL has changed dramatically over the years, care must be taken to ensure we do not include drafts from too long ago. The primary concern with including data from too many years ago is that teams have likely changed their drafting approach over time. For example, teams may have become better at evaluating prospects as more advanced statistics have been developed, meaning that there are likely fewer late round draft “steals” in the 2020s than there were in the 1980s. Thus including drafts from the 1980s would skew our calculations because it would overestimate contributions by players who were drafted in the later rounds, since those players would potentially have been drafted sooner if the teams of the 1980s had the resources available to teams today. This would make our model a poor estimator of draft pick value for drafts occurring in the 2020s. That being said, players drafted in recent years have not had sufficient time to contribute to their teams, so we should not include drafts from too recently either. Ideally, we would wait until all players from a draft class have retired before including it in our analysis . Practically speaking, this is not feasible since players can have very long careers (for example, Alex Ovechkin was drafted in 2004 and is still playing) which would force us to include older drafts to maintain the same sample size, which is also not ideal as explained above.\nHaving considered this, we make the somewhat arbitrary decision to use the 25 drafts between and 1996 and 2020 (inclusive). Note that a significant portion of the players in our dataset are still active, so we will have to adjust for this, which will be done in the Transform chapter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Import</span>"
    ]
  },
  {
    "objectID": "import.html#code",
    "href": "import.html#code",
    "title": "3  Import",
    "section": "3.3 Code",
    "text": "3.3 Code\nWe start off by creating a function to import data from Hockey Reference.\n\n\nCode\nstart_year &lt;- 1996\nend_year &lt;- 2020\n\nimport_draft &lt;- function(year){\n  url &lt;- str_glue(\"https://www.hockey-reference.com/draft/NHL_{year}_entry.html\")\n  html &lt;- read_html(url)\n  Sys.sleep(5) # to avoid getting rate limited\n  draft_year_table &lt;- html |&gt; \n    html_element(\"table\") |&gt; \n    html_table() |&gt; \n    janitor::row_to_names(1) |&gt; \n    janitor::clean_names()\n  draft_year_table\n}\n\ngt(head(import_draft(start_year), 10))\n\n\n\n\n\n\n\n\noverall\nteam\nplayer\nnat\npos\nage\nto\namateur_team\ngp\ng\na\npts\nx\npim\ngp_2\nw\nl\nt_o\nsv_percent\ngaa\nps\n\n\n\n\n1\nOttawa Senators\nChris Phillips\nCA\nD\n18\n2015\nPrince Albert Raiders (WHL)\n1179\n71\n217\n288\n68\n756\n\n\n\n\n\n\n64.6\n\n\n2\nSan Jose Sharks\nAndrei Zyuzin\nRU\nD\n18\n2008\nSalavat Yulaev Ufa (Russia)\n496\n38\n82\n120\n-40\n446\n\n\n\n\n\n\n25.8\n\n\n3\nNew York Islanders\nJ.P. Dumont\nCA\nRW\n18\n2011\nVal-d'Or Foreurs (QMJHL)\n822\n214\n309\n523\n-2\n364\n\n\n\n\n\n\n56.6\n\n\n4\nWashington Capitals\nAlexandre Volchkov\nRU\nC\n18\n2000\nBarrie Colts (OHL)\n3\n0\n0\n0\n-2\n0\n\n\n\n\n\n\n-0.1\n\n\n5\nDallas Stars\nRic Jackman\nCA\nD\n18\n2007\nSoo Greyhounds (OHL)\n231\n19\n58\n77\n-54\n166\n\n\n\n\n\n\n8.8\n\n\n6\nEdmonton Oilers\nBoyd Devereaux\nCA\nC\n18\n2009\nKitchener Rangers (OHL)\n627\n67\n112\n179\n5\n205\n\n\n\n\n\n\n12.5\n\n\n7\nBuffalo Sabres\nErik Rasmussen\nUS\nLW/C\n19\n2007\nMinnesota (WCHA)\n545\n52\n76\n128\n5\n305\n\n\n\n\n\n\n9.2\n\n\n8\nBoston Bruins\nJohnathan Aitken\nCA\nD\n18\n2004\nMedicine Hat Tigers (WHL)\n44\n0\n1\n1\n-12\n70\n\n\n\n\n\n\n0.0\n\n\n9\nAnaheim Ducks\nRuslan Salei\nBY\nD\n21\n2011\nLas Vegas Thunder (IHL)\n917\n45\n159\n204\n-25\n1065\n\n\n\n\n\n\n46.9\n\n\n10\nNew Jersey Devils\nLance Ward\nCA\nD\n18\n2004\nRed Deer Rebels (WHL)\n209\n4\n12\n16\n-30\n391\n\n\n\n\n\n\n2.7\n\n\n\n\n\n\n\nWe compare the first 10 rows of the 1996 draft table shown above with the table on Hockey Reference, and it seems that the function we created does what we want it to do. We can now proceed to the Tidy step.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Import</span>"
    ]
  },
  {
    "objectID": "tidy.html",
    "href": "tidy.html",
    "title": "4  Tidy",
    "section": "",
    "text": "4.1 Setup\nWe install and load the necessary packages, along with functions from prior chapters.\nCode\n# install.packages(\"rvest\")\n# install.packages(\"tidyverse\")\n# install.packages(\"gt\")\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(gt)\n\nsource(\"functions.R\") # load functions defined in prior chapters\n\nstart_year &lt;- 1996\nend_year &lt;- 2020",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy</span>"
    ]
  },
  {
    "objectID": "tidy.html#introduction",
    "href": "tidy.html#introduction",
    "title": "4  Tidy",
    "section": "4.2 Introduction",
    "text": "4.2 Introduction\nIn the tidy step, we put the data into tidy form and clean it, which will make the data easier to analyze in the later steps. Despite the table from the previous chapter looking fairly clean, further inspection reveals some issues:\n\n\nCode\ngt(import_draft(start_year)[23:30,])\n\n\n\n\n\n\n\n\noverall\nteam\nplayer\nnat\npos\nage\nto\namateur_team\ngp\ng\na\npts\nx\npim\ngp_2\nw\nl\nt_o\nsv_percent\ngaa\nps\n\n\n\n\n23\nPittsburgh Penguins\nCraig Hillier\nCA\nG\n\n\nOttawa 67's (OHL)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n24\nPhoenix Coyotes\nDaniel Briere\nCA\nC\n18\n2015\nDrummondville Voltigeurs (QMJHL)\n973\n307\n389\n696\n-24\n744\n\n\n\n\n\n\n78.5\n\n\n25\nColorado Avalanche\nPeter Ratchuk\nUS\nD\n19\n2001\nShattuck-St. Mary's School (High-MN)\n32\n1\n1\n2\n-2\n10\n\n\n\n\n\n\n0.6\n\n\n26\nDetroit Red Wings\nJesse Wallin\nCA\nD\n18\n2003\nRed Deer Rebels (WHL)\n49\n0\n2\n2\n-5\n34\n\n\n\n\n\n\n0.2\n\n\n\nRound 2\nRound 2\n\n\n\n\n\nNHL Scoring Stats\nNHL Scoring Stats\nNHL Scoring Stats\nNHL Scoring Stats\nNHL Scoring Stats\nNHL Scoring Stats\nNHL Goalie Stats\nNHL Goalie Stats\nNHL Goalie Stats\nNHL Goalie Stats\nNHL Goalie Stats\nNHL Goalie Stats\n\n\n\nOverall\nTeam\nPlayer\nNat.\nPos\nAge\nTo\nAmateur Team\nGP\nG\nA\nPTS\n+/-\nPIM\nGP\nW\nL\nT/O\nSV%\nGAA\nPS\n\n\n27\nBuffalo Sabres\nCory Sarich\nCA\nD\n18\n2014\nSaskatoon Blades (WHL)\n969\n21\n137\n158\n-9\n1089\n\n\n\n\n\n\n36.0\n\n\n28\nPittsburgh Penguins\nPavel Skrbek\nCZ\nD\n18\n2002\nHC Kladno (Czech)\n12\n0\n0\n0\n1\n8\n\n\n\n\n\n\n0.2\n\n\n\n\n\n\n\nThree problems that immediately come up are:\n\nTwo rows get inserted at the end of every round to indicate the round changed.\nThough not visible beacuse of the usage of gt, numbers are being coded as strings (overall, age, to, etc).\n(At least) one player is missing everything except for their pick number, name, team, position, nationality, and amateur team.\n\nBy doing a little bit of detective work with some of the other players with missing values elsewhere in the dataset, we notice that players who never played in the NHL have empty strings listed for everything except for the values attributes listed above. We will have to deal with this in the tidy step. Note that Hockey Reference begins listing player’s ages in the 2001 draft, but we aren’t going to use ages for our analysis so we won’t bother coming up for a remedy for the players drafted between 1996 and 2000. The number of picks in the draft has also changed over the years, we will ignore any player drafted after selection 224 (the number of picks in the 2025 NHL Entry Draft). Finally, it would be helpful to remove the columns we don’t care about.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy</span>"
    ]
  },
  {
    "objectID": "tidy.html#code",
    "href": "tidy.html#code",
    "title": "4  Tidy",
    "section": "4.3 Code",
    "text": "4.3 Code\nWe build a function to tidy the data. In particular, we want it to:\n\nRemove the rows added between rounds.\nCorrect the types of each column so we can use numeric columns in calculations.\nChange gp and ps values to 0 for players who never played in the NHL or have a negative ps.\nIf is.na(to), then the player never played in the NHL, so set it to the draft year.\nAdd a year column so we can adjust the stats of players drafted more recently.\nSelect the columns we care about (year, overall, pos, to, gp, and ps) in that order.\nRemove any players selected after 224\\(^{\\text{th}}\\) overall.\n\nNote that there were originally two gp columns (one for games played and one for games played as a goalie, goalies have the same number in both), but when we used janitor::clean_names() it changed them to gp and gp_2. Additionally, we cannot remove the round separating rows by removing a specified row number since many of the drafts in our dataset have different numbers of picks per round, and some rounds within the same draft have even had a different numbers of picks.\n\n\nCode\ntidy_draft &lt;- function(year){\n  draft_year_table &lt;- import_draft(year) |&gt; \n    filter(overall != \"Overall\" & overall != \"\" & as.numeric(overall) &lt; 225) |&gt; # remove extra rows and players after pick 224\n    type_convert() |&gt; # fix types \n    mutate(\"year\" = year, \"ps\" = pmax(coalesce(ps, 0), 0), \n           \"gp\" = coalesce(gp, 0), \"to\" = coalesce(to, year)) |&gt; \n    select(year, overall, to, pos, gp, ps) # columns we care about\n  draft_year_table\n}\n\ngt(tidy_draft(1996))\n\n\n\n\n\n\n\n\nyear\noverall\nto\npos\ngp\nps\n\n\n\n\n1996\n1\n2015\nD\n1179\n64.6\n\n\n1996\n2\n2008\nD\n496\n25.8\n\n\n1996\n3\n2011\nRW\n822\n56.6\n\n\n1996\n4\n2000\nC\n3\n0.0\n\n\n1996\n5\n2007\nD\n231\n8.8\n\n\n1996\n6\n2009\nC\n627\n12.5\n\n\n1996\n7\n2007\nLW/C\n545\n9.2\n\n\n1996\n8\n2004\nD\n44\n0.0\n\n\n1996\n9\n2011\nD\n917\n46.9\n\n\n1996\n10\n2004\nD\n209\n2.7\n\n\n1996\n11\n2004\nD\n82\n0.0\n\n\n1996\n12\n2004\nC\n60\n0.6\n\n\n1996\n13\n2014\nD\n1107\n74.6\n\n\n1996\n14\n2013\nC\n798\n17.7\n\n\n1996\n15\n2016\nRW\n1293\n52.0\n\n\n1996\n16\n1999\nD\n5\n0.0\n\n\n1996\n17\n2000\nRW\n113\n3.5\n\n\n1996\n18\n2001\nC\n57\n0.0\n\n\n1996\n19\n2001\nD\n5\n0.3\n\n\n1996\n20\n2008\nLW\n521\n12.8\n\n\n1996\n21\n2012\nLW\n938\n54.0\n\n\n1996\n22\n1996\nD\n0\n0.0\n\n\n1996\n23\n1996\nG\n0\n0.0\n\n\n1996\n24\n2015\nC\n973\n78.5\n\n\n1996\n25\n2001\nD\n32\n0.6\n\n\n1996\n26\n2003\nD\n49\n0.2\n\n\n1996\n27\n2014\nD\n969\n36.0\n\n\n1996\n28\n2002\nD\n12\n0.2\n\n\n1996\n29\n2009\nLW\n337\n0.0\n\n\n1996\n30\n2012\nLW\n341\n5.1\n\n\n1996\n31\n1999\nD\n18\n0.0\n\n\n1996\n32\n2004\nD\n6\n0.0\n\n\n1996\n33\n1996\nLW\n0\n0.0\n\n\n1996\n34\n1996\nLW\n0\n0.0\n\n\n1996\n35\n2019\nC\n1516\n66.0\n\n\n1996\n36\n2001\nD\n19\n0.6\n\n\n1996\n37\n2002\nRW\n73\n2.6\n\n\n1996\n38\n1996\nLW\n0\n0.0\n\n\n1996\n39\n2004\nLW\n55\n0.0\n\n\n1996\n40\n2013\nC\n524\n6.6\n\n\n1996\n41\n1996\nD\n0\n0.0\n\n\n1996\n42\n2003\nD\n2\n0.0\n\n\n1996\n43\n2007\nC\n552\n22.0\n\n\n1996\n44\n2013\nG\n341\n49.0\n\n\n1996\n45\n1996\nRW\n0\n0.0\n\n\n1996\n46\n1996\nC\n0\n0.0\n\n\n1996\n47\n2006\nRW\n142\n7.3\n\n\n1996\n48\n2000\nLW\n53\n0.7\n\n\n1996\n49\n2012\nD\n797\n42.1\n\n\n1996\n50\n1996\nG\n0\n0.0\n\n\n1996\n51\n2001\nC\n3\n0.0\n\n\n1996\n52\n1996\nG\n0\n0.0\n\n\n1996\n53\n1996\nLW\n0\n0.0\n\n\n1996\n54\n1996\nC\n0\n0.0\n\n\n1996\n55\n1996\nG\n0\n0.0\n\n\n1996\n56\n2022\nD\n1680\n155.1\n\n\n1996\n57\n1996\nRW\n0\n0.0\n\n\n1996\n58\n1996\nD\n0\n0.0\n\n\n1996\n59\n2013\nD\n824\n58.9\n\n\n1996\n60\n1999\nD\n2\n0.1\n\n\n1996\n61\n1996\nRW\n0\n0.0\n\n\n1996\n62\n1996\nD\n0\n0.0\n\n\n1996\n63\n2008\nRW\n308\n0.3\n\n\n1996\n64\n1996\nRW\n0\n0.0\n\n\n1996\n65\n2006\nLW/C\n493\n19.5\n\n\n1996\n66\n1996\nD\n0\n0.0\n\n\n1996\n67\n2004\nLW\n108\n0.0\n\n\n1996\n68\n1996\nLW\n0\n0.0\n\n\n1996\n69\n1996\nRW\n0\n0.0\n\n\n1996\n70\n2011\nLW\n469\n11.3\n\n\n1996\n71\n2014\nRW\n789\n16.8\n\n\n1996\n72\n2010\nLW\n31\n0.0\n\n\n1996\n73\n1996\nLW\n0\n0.0\n\n\n1996\n74\n1996\nG\n0\n0.0\n\n\n1996\n75\n2004\nD\n21\n0.5\n\n\n1996\n76\n1996\nLW\n0\n0.0\n\n\n1996\n77\n1996\nRW\n0\n0.0\n\n\n1996\n78\n1996\nC\n0\n0.0\n\n\n1996\n79\n2011\nRW\n722\n42.6\n\n\n1996\n80\n1996\nRW\n0\n0.0\n\n\n1996\n81\n2002\nD\n29\n0.4\n\n\n1996\n82\n2004\nRW\n73\n0.5\n\n\n1996\n83\n1999\nG\n3\n0.0\n\n\n1996\n84\n1996\nC\n0\n0.0\n\n\n1996\n85\n1996\nRW\n0\n0.0\n\n\n1996\n86\n1996\nRW\n0\n0.0\n\n\n1996\n87\n1996\nRW\n0\n0.0\n\n\n1996\n88\n2009\nLW\n233\n0.5\n\n\n1996\n89\n2013\nD\n847\n51.5\n\n\n1996\n90\n1996\nRW\n0\n0.0\n\n\n1996\n91\n2004\nD\n47\n0.4\n\n\n1996\n92\n1996\nC\n0\n0.0\n\n\n1996\n93\n1996\nRW\n0\n0.0\n\n\n1996\n94\n1996\nD\n0\n0.0\n\n\n1996\n95\n1996\nC\n0\n0.0\n\n\n1996\n96\n2013\nC\n820\n28.9\n\n\n1996\n97\n1996\nRW\n0\n0.0\n\n\n1996\n98\n1996\nD\n0\n0.0\n\n\n1996\n99\n1996\nC\n0\n0.0\n\n\n1996\n100\n2012\nC\n194\n0.8\n\n\n1996\n101\n1996\nD\n0\n0.0\n\n\n1996\n102\n2012\nRW\n675\n8.1\n\n\n1996\n103\n1996\nRW\n0\n0.0\n\n\n1996\n104\n1996\nC\n0\n0.0\n\n\n1996\n105\n2017\nD\n963\n68.2\n\n\n1996\n106\n1996\nD\n0\n0.0\n\n\n1996\n107\n1996\nG\n0\n0.0\n\n\n1996\n108\n1996\nLW\n0\n0.0\n\n\n1996\n109\n2003\nD\n37\n1.8\n\n\n1996\n110\n1996\nC\n0\n0.0\n\n\n1996\n111\n1996\nRW\n0\n0.0\n\n\n1996\n112\n2002\nLW\n7\n0.0\n\n\n1996\n113\n1996\nD\n0\n0.0\n\n\n1996\n114\n1996\nRW\n0\n0.0\n\n\n1996\n115\n2002\nD\n30\n0.6\n\n\n1996\n116\n1996\nD\n0\n0.0\n\n\n1996\n117\n1996\nD\n0\n0.0\n\n\n1996\n118\n1996\nC\n0\n0.0\n\n\n1996\n119\n2003\nD\n112\n3.9\n\n\n1996\n120\n1996\nD\n0\n0.0\n\n\n1996\n121\n1996\nC\n0\n0.0\n\n\n1996\n122\n1996\nC\n0\n0.0\n\n\n1996\n123\n1996\nD\n0\n0.0\n\n\n1996\n124\n1996\nG\n0\n0.0\n\n\n1996\n125\n1996\nD\n0\n0.0\n\n\n1996\n126\n1996\nLW\n0\n0.0\n\n\n1996\n127\n1996\nD\n0\n0.0\n\n\n1996\n128\n1996\nLW\n0\n0.0\n\n\n1996\n129\n1996\nC\n0\n0.0\n\n\n1996\n130\n1996\nD\n0\n0.0\n\n\n1996\n131\n1996\nLW\n0\n0.0\n\n\n1996\n132\n1996\nD\n0\n0.0\n\n\n1996\n133\n2009\nRW\n172\n0.1\n\n\n1996\n134\n1996\nLW\n0\n0.0\n\n\n1996\n135\n1996\nG\n0\n0.0\n\n\n1996\n136\n2004\nRW\n613\n19.9\n\n\n1996\n137\n2001\nG\n3\n0.0\n\n\n1996\n138\n1996\nC\n0\n0.0\n\n\n1996\n139\n2007\nG\n186\n24.9\n\n\n1996\n140\n2000\nD\n2\n0.0\n\n\n1996\n141\n1996\nC\n0\n0.0\n\n\n1996\n142\n1996\nRW\n0\n0.0\n\n\n1996\n143\n1996\nG\n0\n0.0\n\n\n1996\n144\n1996\nRW\n0\n0.0\n\n\n1996\n145\n1996\nRW\n0\n0.0\n\n\n1996\n146\n2011\nRW\n381\n5.9\n\n\n1996\n147\n1996\nG\n0\n0.0\n\n\n1996\n148\n1996\nD\n0\n0.0\n\n\n1996\n149\n1996\nG\n0\n0.0\n\n\n1996\n150\n1996\nC\n0\n0.0\n\n\n1996\n151\n1996\nLW\n0\n0.0\n\n\n1996\n152\n1996\nD\n0\n0.0\n\n\n1996\n153\n1996\nRW\n0\n0.0\n\n\n1996\n154\n2013\nD\n689\n31.7\n\n\n1996\n155\n1996\nD\n0\n0.0\n\n\n1996\n156\n1996\nLW\n0\n0.0\n\n\n1996\n157\n2001\nC\n16\n0.4\n\n\n1996\n158\n1996\nD\n0\n0.0\n\n\n1996\n159\n1996\nG\n0\n0.0\n\n\n1996\n160\n1996\nG\n0\n0.0\n\n\n1996\n161\n1996\nC\n0\n0.0\n\n\n1996\n162\n1996\nC\n0\n0.0\n\n\n1996\n163\n1996\nD\n0\n0.0\n\n\n1996\n164\n1996\nD\n0\n0.0\n\n\n1996\n165\n1996\nG\n0\n0.0\n\n\n1996\n166\n1996\nG\n0\n0.0\n\n\n1996\n167\n2009\nRW\n503\n3.1\n\n\n1996\n168\n1996\nC\n0\n0.0\n\n\n1996\n169\n2004\nC\n77\n2.6\n\n\n1996\n170\n1996\nRW\n0\n0.0\n\n\n1996\n171\n2001\nD\n1\n0.0\n\n\n1996\n172\n1996\nD\n0\n0.0\n\n\n1996\n173\n1996\nD\n0\n0.0\n\n\n1996\n174\n2008\nRW\n616\n14.5\n\n\n1996\n175\n1996\nD\n0\n0.0\n\n\n1996\n176\n2012\nC\n798\n4.0\n\n\n1996\n177\n2007\nRW\n256\n0.0\n\n\n1996\n178\n1996\nC\n0\n0.0\n\n\n1996\n179\n2012\nD\n970\n64.1\n\n\n1996\n180\n1996\nRW\n0\n0.0\n\n\n1996\n181\n1996\nRW\n0\n0.0\n\n\n1996\n182\n1996\nD\n0\n0.0\n\n\n1996\n183\n1996\nD\n0\n0.0\n\n\n1996\n184\n1996\nD\n0\n0.0\n\n\n1996\n185\n1996\nD\n0\n0.0\n\n\n1996\n186\n2007\nRW\n74\n0.8\n\n\n1996\n187\n1996\nLW\n0\n0.0\n\n\n1996\n188\n1996\nC\n0\n0.0\n\n\n1996\n189\n1996\nC\n0\n0.0\n\n\n1996\n190\n2010\nG\n46\n6.1\n\n\n1996\n191\n1996\nC\n0\n0.0\n\n\n1996\n192\n2002\nD\n42\n0.4\n\n\n1996\n193\n2001\nLW\n69\n2.6\n\n\n1996\n194\n2008\nD\n282\n8.2\n\n\n1996\n195\n2011\nRW\n462\n15.8\n\n\n1996\n196\n2004\nC\n8\n0.1\n\n\n1996\n197\n1996\nLW\n0\n0.0\n\n\n1996\n198\n1996\nD\n0\n0.0\n\n\n1996\n199\n2016\nD\n907\n58.0\n\n\n1996\n200\n1996\nRW\n0\n0.0\n\n\n1996\n201\n1996\nC\n0\n0.0\n\n\n1996\n202\n1996\nRW\n0\n0.0\n\n\n1996\n203\n1996\nC\n0\n0.0\n\n\n1996\n204\n2013\nD\n984\n87.0\n\n\n1996\n205\n1996\nRW\n0\n0.0\n\n\n1996\n206\n1996\nD\n0\n0.0\n\n\n1996\n207\n1996\nF\n0\n0.0\n\n\n1996\n208\n1996\nRW\n0\n0.0\n\n\n1996\n209\n1996\nG\n0\n0.0\n\n\n1996\n210\n1996\nD\n0\n0.0\n\n\n1996\n211\n1996\nD\n0\n0.0\n\n\n1996\n212\n2000\nD\n1\n0.0\n\n\n1996\n213\n1996\nC\n0\n0.0\n\n\n1996\n214\n1996\nD\n0\n0.0\n\n\n1996\n215\n1996\nRW\n0\n0.0\n\n\n1996\n216\n2004\nRW\n89\n2.7\n\n\n1996\n217\n1996\nLW\n0\n0.0\n\n\n1996\n218\n1996\nD\n0\n0.0\n\n\n1996\n219\n1996\nLW\n0\n0.0\n\n\n1996\n220\n1996\nLW\n0\n0.0\n\n\n1996\n221\n1996\nG\n0\n0.0\n\n\n1996\n222\n1996\nG\n0\n0.0\n\n\n1996\n223\n2015\nRW\n951\n1.9\n\n\n1996\n224\n1996\nLW\n0\n0.0\n\n\n\n\n\n\n\nThis is the form we will use for analysis later. We now load in all of the data and use rbind() to bind the tables together, giving us a single data frame to work with. This also means that our data will be in a mix of long and wide format, since the year and overall columns are formatted like they are in long format but the to, pos, gp, and ps columns are the same as they would be in a wide format. Note that this function takes around 4 minutes to run (because of the Sys.sleep(5) line, which is necessary to prevent us getting rate limited). We will find a workaround for this shortly so we don’t have to run this function any more than we have to.\n\n\nCode\nall_data &lt;- do.call(rbind, lapply(seq(start_year, end_year, 1),\n                   \\(x) tidy_draft(x)))\n\n\nWe can check that this data has loaded correctly, there should be between 5000 and 6000 rows (the number of picks in a draft has changed over the years, but is usually between 200 and 240, so 25 drafts will be between 5000 and 6000) and 6 columns:\n\n\nCode\ndim(all_data) # confirm there are 5000-6000 rows and 6 columns\n\n\n[1] 5427    6\n\n\nCode\nlength(unique(all_data$year)) # confirm all 25 years have been included\n\n\n[1] 25\n\n\nCode\ngt(head(all_data, 10))\n\n\n\n\n\n\n\n\nyear\noverall\nto\npos\ngp\nps\n\n\n\n\n1996\n1\n2015\nD\n1179\n64.6\n\n\n1996\n2\n2008\nD\n496\n25.8\n\n\n1996\n3\n2011\nRW\n822\n56.6\n\n\n1996\n4\n2000\nC\n3\n0.0\n\n\n1996\n5\n2007\nD\n231\n8.8\n\n\n1996\n6\n2009\nC\n627\n12.5\n\n\n1996\n7\n2007\nLW/C\n545\n9.2\n\n\n1996\n8\n2004\nD\n44\n0.0\n\n\n1996\n9\n2011\nD\n917\n46.9\n\n\n1996\n10\n2004\nD\n209\n2.7\n\n\n\n\n\n\n\nCode\ngt(all_data[23:30,])\n\n\n\n\n\n\n\n\nyear\noverall\nto\npos\ngp\nps\n\n\n\n\n1996\n23\n1996\nG\n0\n0.0\n\n\n1996\n24\n2015\nC\n973\n78.5\n\n\n1996\n25\n2001\nD\n32\n0.6\n\n\n1996\n26\n2003\nD\n49\n0.2\n\n\n1996\n27\n2014\nD\n969\n36.0\n\n\n1996\n28\n2002\nD\n12\n0.2\n\n\n1996\n29\n2009\nLW\n337\n0.0\n\n\n1996\n30\n2012\nLW\n341\n5.1\n\n\n\n\n\n\n\nThese checks all returned what they should. Finally, recall that the import_data function (and thus the tidy_draft function) both take quite a while to run, especially when importing data for a large number of years. To resolve this issue, we create a csv file to store the data so future chapters can pull data from the csv file instead of having to get it from Hockey Reference again.\n\n\nCode\nwrite.csv(all_data, \"all_data.csv\", row.names = FALSE)\n\n\nIn future chapters, we will import data from this csv file to speed up the run and rendering times (instead of importing it from Hockey Reference every time we simply load it from the csv). Now we proceed to the Visualize step.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidy</span>"
    ]
  },
  {
    "objectID": "visualize.html",
    "href": "visualize.html",
    "title": "5  Visualize",
    "section": "",
    "text": "5.1 Setup\nWe install and load the necessary packages, along with functions from prior chapters and all_data.\nCode\n# install.packages(\"gt\")\n# install.packages(\"ggplot2\")\nlibrary(gt)\nlibrary(ggplot2)\nsource(\"functions.R\") # load functions defined in prior chapters\n\nall_data &lt;- read.csv(\"all_data.csv\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualize</span>"
    ]
  },
  {
    "objectID": "visualize.html#introduction",
    "href": "visualize.html#introduction",
    "title": "5  Visualize",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction\nIn the visualize step, we will perform some EDA (Exploratory Data Analysis) to get a sense of what our data looks like. Specifically, we will see if there are any patterns or trends that may be useful in the Model chapter.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualize</span>"
    ]
  },
  {
    "objectID": "visualize.html#code",
    "href": "visualize.html#code",
    "title": "5  Visualize",
    "section": "5.3 Code",
    "text": "5.3 Code\nRecall that over the years the NHL has changed how many picks there are in each round as franchises have been added. A consequence of this is that the number of rounds has also changed, and the number of total picks in a draft has changed several times throughout our dataset. Recall that we removed all picks after #224, but there could be drafts with fewer than 224 total selections. We check for this:\n\n\nCode\nall_data |&gt; \n  group_by(year) |&gt; \n  summarize(num_picks = n()) |&gt; \n  gt()\n\n\n\n\n\n\n\n\nyear\nnum_picks\n\n\n\n\n1996\n224\n\n\n1997\n224\n\n\n1998\n224\n\n\n1999\n224\n\n\n2000\n224\n\n\n2001\n224\n\n\n2002\n224\n\n\n2003\n224\n\n\n2004\n224\n\n\n2005\n224\n\n\n2006\n213\n\n\n2007\n211\n\n\n2008\n211\n\n\n2009\n210\n\n\n2010\n210\n\n\n2011\n211\n\n\n2012\n211\n\n\n2013\n211\n\n\n2014\n210\n\n\n2015\n211\n\n\n2016\n211\n\n\n2017\n217\n\n\n2018\n217\n\n\n2019\n217\n\n\n2020\n216\n\n\n\n\n\n\n\nIndeed the drafts after 2005 all have fewer than 224 selections. This is not a major problem since those picks aren’t worth very much anyway, but it is worth noting that several picks after #210 have a smaller sample size than picks 1-210.\nBefore doing any further EDA, we will take the five number summary, mean, and standard deviation of both the GP and PS values to get a sense of what they look like. Recall that the five number summary gives the minimum, 25% quantile, median, 75% quantile, and maximum of a dataset. Additionally, recall that PS stands for Point Share, and is a measure of a player’s career contributions to points in the standings (ie the points you get from wins, not the one that is goals plus assists).\n\n\nCode\nc(fivenum(all_data$gp), mean(all_data$gp), sd(all_data$gp))\n\n\n[1]    0.0000    0.0000    0.0000  145.0000 1779.0000  142.7713  274.0303\n\n\nCode\nc(fivenum(all_data$ps), mean(all_data$ps), sd(all_data$ps))\n\n\n[1]   0.000000   0.000000   0.000000   3.000000 217.800000   8.249862  21.182591\n\n\nClearly both the GP and PS values are right skewed. Note that the maximum of the GP data is around 6 standard deviations from the mean \\((\\frac{1779-142.7713}{274.0303} = 5.97)\\) , whereas the maximum of the PS data is almost 10 standard deviations away \\((\\frac{217.8-8.249862}{21.172591} = 9.89)\\).\nWe next check what proportion of our dataset ever played in an NHL game and what proportion generated less than 2 PS in their career, which is the value of exactly one win in the NHL.\n\n\nCode\nall_data |&gt; \n  filter(gp &gt; 0) |&gt; \n  nrow() / nrow(all_data)\n\n\n[1] 0.481113\n\n\nCode\nall_data |&gt; \n  filter(ps &lt; 2) |&gt; \n  nrow() / nrow(all_data)\n\n\n[1] 0.7280265\n\n\nThis tells us that just over half of our dataset never played in an NHL game and almost three quarters generated less than 2 PS in their career.\nTo get visual confirmation that our data is very right skewed, we check the histograms of the data, one of GP (on the left) and one of PS (on the right). Note that we have to pivot the data and put it in long format first:\n\n\nCode\nall_data |&gt; pivot_longer(cols = c(gp, ps),\n                         names_to = \"metric\", \n                         values_to = \"value\") |&gt; \n  ggplot(aes(value)) + \n  geom_histogram() + \n  facet_wrap(~metric, scales = \"free\") + \n  labs(title = \"Distribution of GP and PS\", \n       subtitle = \"Note the plots have different scales\", \n       x = \"Value\", y = \"Number of Players\") \n\n\n\n\n\n\n\n\n\nIndeed, both of these are very right-skewed, and clearly a lot of players end up playing a small number of games and are thus not generating able to generate much PS. We may also guess that GP and PS are positively correlated, since better players get to play in more games and thus accumulate more PS. Indeed:\n\n\nCode\ncor(all_data$ps, all_data$gp)\n\n\n[1] 0.8558384\n\n\nBecause of this, we choose to only include one of GP and PS in our model to avoid potential multicollinearity concerns. We choose PS over GP for a few reasons. First, PS credits players for contributing to their team, whereas GP gives credit for being good enough to play for a team. Second, though both metrics are right skewed I would argue that in this context we would prefer a metric which has a longer right tail since this will allow us to distinguish good players from elite players. Specifically, there is a hard cap on how many games a player can play in a certain timeframe, but the limit on PS is impossible to reach (a player would have to win every game in his career and be fully responsible for each and every win). In other words, if two players each played in 82 games per season for 10 seasons before retiring, they would both have played in 820 games, but their PS values could be quite different, indicating that PS is a more distingushing metric. The final reason we will use PS is that the formula for PS does include time on ice, which tends to be a better measure of player involvement than GP. For example, Player A who plays 20 minutes a night and and Player B who plays 10 minutes a night may have the same GP, but Player A would likely be considered more valuable because he plays twice as much.\nFinally, we wish to confirm that players selected earlier in a draft (ie a lower overall) tend to generate more PS in their careers than those selected later. To check this, we start by creating a plot of the PS values by overall for a single draft.\n\n\nCode\nset.seed(468) # for reproducibility\nrand_year &lt;- sample(start_year:end_year, 1) # year is 2015\n\nall_data |&gt; \n  filter(year == rand_year) |&gt; \n  ggplot(aes(x = overall, y = ps)) +\n  geom_point() +\n  labs(x = \"Pick Number\", y = \"PS\", \n       title = str_glue(\"PS of Players Drafted in {rand_year}\"))\n\n\n\n\n\n\n\n\n\nSince clearly so many players never play in the NHL, we will create the plot without the players that played in 0 NHL games to make the plot easier to read.\n\n\nCode\nall_data |&gt; \n  filter(gp &gt; 0) |&gt; \n  ggplot(aes(x = overall, y = ps)) + \n  geom_point(alpha = 0.5) + \n  labs(title = \"Distribution of PS\",\n       subtitle = \"Players with at ≥ 1 game only; note the plots have different scales\", \n       x = \"PS\", y = \"Number of Players\") \n\n\n\n\n\n\n\n\n\nThis plot is quite dense and difficult to interpret, but there isn’t really any point in jittering the data because it’ll still overlap, so we re plot it with a random 5 year sample of our dataset.\n\n\nCode\nyears &lt;- sample(start_year:end_year, 5) # years are 2018, 1996, 1999, 2010, 2004\n\nall_data |&gt; \n  filter(gp &gt; 0 & year %in% years) |&gt; \n  ggplot(aes(x = overall, y = ps)) + \n  geom_point(alpha = 0.5) + \n  labs(title = \"Distribution of PS for 5 Drafts\",\n       subtitle = \"Players with at ≥ 1 game only; note the plots have different scales\", \n       x = \"PS\", y = \"Number of Players\")\n\n\n\n\n\n\n\n\n\nThough this plot is still quite busy, it shows a strange trend that there seems to be a lot of players drafted around 200 overall that end up having successful careers. I am not sure of an underlying reason for this, but we will need to be careful when modeling to ensure that these late picks are not given more value than earlier picks.\nThe plot above is not great because it is missing the vast majority of our dataset (20 drafts plus all the players who played in 0 NHL games for the 5 years in the sample). To improve this, we plot the mean PS of the players selected at each pick. Typically for such skewed data the median would be preferred over the mean to improve resistance to outliers. However in this context we don’t really want to be resistance to outliers since the outliers represent elite players, and ignoring those players would make our model underestimate the values of picks which have been used to select extremely successful players.\n\n\nCode\nall_data |&gt; \n  group_by(overall) |&gt; \n  summarize(mean_ps = mean(ps)) |&gt; \n  ggplot(aes(x = overall, y = mean_ps)) + \n  geom_point() +\n  geom_point(aes(x = 205, y = mean(filter(all_data, overall==205)$ps)), col = \"salmon\") +\n  labs(title = \"Mean Points Share by Pick Number\",\n       x = \"Pick Number\", y = \"Mean Points Share\") + \n  annotate(geom = \"segment\", x = 175, y = 37.5, xend = 203, yend = 14, colour = \"salmon\",\n    arrow = arrow(type = \"open\", length = unit(0.32, \"cm\"))) +\n  annotate(geom = \"label\", x = 140, y = 39,\n    label = \"205th overall selection,\\n (mean PS of 13.124)\",\n    hjust = \"left\", colour = \"salmon\")\n\n\n\n\n\n\n\n\n\nWe can see that in general PS does tend to decrease later in drafts, and that pick value tends to level off around pick 75, though there are some picks that stick out (for example, pick 205 has an average PS of 13.124, whereas pick 204 has an average PS of 4.856). This particular outlier is due to Henrik Lundqvist and Joe Pavelski being selected at this spot and having career PSs of 173.3 and 130.1, respectively. Very few players drafted this late make it to the NHL (20 of the 25 players in our dataset drafted at pick 205 have more than 0.3 PS), so two players with very successful careers skewing the data is not surprising.\nThe lessons to take from this plot when creating a model are that it will be necessary to fit a smooth curve to the model (so that \\(v_i &gt; v_{i+c}\\) where \\(c \\in \\mathbb Z^+\\)), and that the estimated value of pick \\(n\\) should probably be influenced by the historical value of pick \\(n\\) AND the historical value of picks “close” to \\(n\\). With this in mind, we proceed to the Transform step.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualize</span>"
    ]
  },
  {
    "objectID": "transform.html",
    "href": "transform.html",
    "title": "6  Transform",
    "section": "",
    "text": "6.1 Setup\nWe install and load the necessary packages, along with functions from prior chapters and all_data.\nCode\n# install.packages(\"tidyverse\")\n# install.packages(\"dplyr\")\n# install.packages(\"gt\")\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(gt)\n\nsource(\"functions.R\") # load functions defined in prior chapters\n\nall_data &lt;- read.csv(\"all_data.csv\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transform</span>"
    ]
  },
  {
    "objectID": "transform.html#introduction",
    "href": "transform.html#introduction",
    "title": "6  Transform",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nIn the transform step, we will transform our data to better include the careers of players who are still playing. In the previous chapter, we estimated the value of pick \\(n\\) by taking the average points share of all players in out dataset drafted at pick \\(n\\). One problem with this approach is that players drafted more recently will have had fewer years to generate points share. Indeed, the average points share is quite a bit lower for the drafts between 2016 and 2020:\n\n\nCode\nall_data |&gt; \n  group_by(year) |&gt; \n  summarize(avg_ps = mean(ps)) |&gt;\n  ggplot(aes(x = year, y = avg_ps)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nThis will result in older drafts receiving more weight when we take the average, which is unideal because we’d like all drafts to be equally weighted to prevent outliers from receiving more weight than they should. To adjust this, we will change our metric slightly, and will instead use \\(p_{ij} = \\frac{ps_{ij}}{\\sum_i ps_{ij}}\\), where \\(ps_{ij}\\) is the points share of the player picked at selection \\(i\\) in year \\(j\\). In other words, we define \\(p_{ij}\\) to be the percentage of the total points share generated by players drafted in year \\(j\\) that the player selected at pick \\(i\\) generated.\nThe other transformation we will apply to our dataset before the modelling step addresses one of points we mentioned at the very end of the EDA we performed in the Visualize chapter. Specifically, we will apply a weighted \\(k\\)-nearest neighbour algorithm so that the value of pick \\(n\\) will be more directly influenced by the historical outcomes of picks “close” to pick \\(n\\). Note that I learned how to inplemment a weighted \\(k\\)-nearest neighbour algorithm in STAT341.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transform</span>"
    ]
  },
  {
    "objectID": "transform.html#code",
    "href": "transform.html#code",
    "title": "6  Transform",
    "section": "6.3 Code",
    "text": "6.3 Code\n\n\nCode\nall_data_prop &lt;- all_data |&gt; \n  group_by(year) |&gt; \n  mutate(prop_ps = ps/sum(ps)) \n\n\nall_data_prop\n\n\n# A tibble: 5,427 × 7\n# Groups:   year [25]\n    year overall    to pos      gp    ps prop_ps\n   &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1  1996       1  2015 D      1179  64.6 0.0407 \n 2  1996       2  2008 D       496  25.8 0.0163 \n 3  1996       3  2011 RW      822  56.6 0.0357 \n 4  1996       4  2000 C         3   0   0      \n 5  1996       5  2007 D       231   8.8 0.00555\n 6  1996       6  2009 C       627  12.5 0.00788\n 7  1996       7  2007 LW/C    545   9.2 0.00580\n 8  1996       8  2004 D        44   0   0      \n 9  1996       9  2011 D       917  46.9 0.0296 \n10  1996      10  2004 D       209   2.7 0.00170\n# ℹ 5,417 more rows\n\n\nNote that usually it is not advisable to take the mean of a bunch of ratios because it gives each ratio an equal weight. However, here it is appropriate because we want to give each draft an equal weight. We will recreate the same plot, but note that the only variation will come from the number of selections in a draft (since what we are plotting is literally \\(\\frac{1}{\\text{number of picks in draft}}\\)). Additionally, the scale on the \\(y\\)-axis shows that these values are basically all the same.\n\n\nCode\nall_data_prop |&gt; \n  group_by(year) |&gt; \n  summarize(avg_ps = mean(prop_ps)) |&gt;\n  ggplot(aes(x = year, y = avg_ps)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nIn case this approach is not appropriate, we will still fit a model using the raw ps values in the Model step, it turns out the models will be quite similar.\n\n\nCode\nall_data_prop &lt;- all_data_prop |&gt; \n  group_by(overall) |&gt;  \n  summarize(avg_prop_ps = mean(prop_ps),\n            .groups = \"drop\")\n\nall_data_prop\n\n\n# A tibble: 224 × 2\n   overall avg_prop_ps\n     &lt;int&gt;       &lt;dbl&gt;\n 1       1      0.0544\n 2       2      0.0427\n 3       3      0.0332\n 4       4      0.0312\n 5       5      0.0247\n 6       6      0.0199\n 7       7      0.0228\n 8       8      0.0135\n 9       9      0.0185\n10      10      0.0119\n# ℹ 214 more rows\n\n\nFinally, we utilize a weighted \\(k\\)-nearest neighbour algorithm which will help smoothen out the plot from the end of the visualization chapter. As a reminder, here is that plot:\n\n\nCode\nall_data_raw &lt;- all_data |&gt; \n  group_by(overall) |&gt; \n  summarize(mean_ps = mean(ps)) \n\nhead(all_data_raw, 10)\n\n\n# A tibble: 10 × 2\n   overall mean_ps\n     &lt;int&gt;   &lt;dbl&gt;\n 1       1    96.2\n 2       2    77.3\n 3       3    54.3\n 4       4    50.3\n 5       5    45.1\n 6       6    33.9\n 7       7    39.2\n 8       8    23.0\n 9       9    32.7\n10      10    17.6\n\n\nCode\nggplot(all_data_raw, aes(x = overall, y = mean_ps)) + \n  geom_point() +\n  geom_point(aes(x = 205, y = mean(filter(all_data, overall==205)$ps)), col = \"salmon\") +\n  labs(title = \"Mean Points Share by Pick Number\",\n       x = \"Pick Number\", y = \"Mean Points Share\") + \n  annotate(geom = \"segment\", x = 175, y = 37.5, xend = 203, yend = 14, colour = \"salmon\",\n    arrow = arrow(type = \"open\", length = unit(0.32, \"cm\"))) +\n  annotate(geom = \"label\", x = 140, y = 39,\n    label = \"205th overall selection,\\n(mean PS of 13.124)\",\n    hjust = \"left\", colour = \"salmon\")\n\n\n\n\n\n\n\n\n\nFirst, we will recreate this plot using the avg_prop_ps values we just defined:\n\n\nCode\nall_data_prop |&gt; \n  ggplot(aes(x = overall, y = avg_prop_ps)) + \n  geom_point() +\n  geom_point(aes(x = 205, y = mean(filter(all_data_prop, overall==205)$avg_prop_ps)), \n             col = \"salmon\") +\n  labs(title = \"Mean Proportion of Points Share by Pick Number\",\n       x = \"Pick Number\", y = \"Mean Proportion of Points Share\") + \n  annotate(geom = \"segment\", x = 175, y = 0.015, xend = 203, yend = 0.0065, colour = \"salmon\",\n    arrow = arrow(type = \"open\", length = unit(0.32, \"cm\"))) +\n  annotate(geom = \"label\", x = 140, y = 0.018,\n    label = \"205th overall selection, (mean\\n0.609% of the draft's ps)\",\n    hjust = \"left\", colour = \"salmon\")\n\n\n\n\n\n\n\n\n\nThe shapes of the two graphs are quite similar, but the outliers in the second one seem slighly less egregious. We now apply the weighted \\(k\\)-nearest neighbour algorithm, first on all_data, and then on all_data_prop. Note that we will choose \\(k\\) to be a function of \\(n\\), specifically we will include any pick which is within \\(\\lfloor \\frac{\\sqrt n}{2}\\rfloor +1\\) of \\(n\\) which means that our estimate of \\(v_1\\) depends on the historical values of just pick 1 and 2, whereas the estimated value of pick 200 depends on the historical values of picks 192-208 (ie any pick \\(i\\) such that \\(| i - 200 | \\le \\lfloor \\frac{\\sqrt {200}}{2}\\rfloor +1 = 8\\)). We also need to choose a weight function, we choose to give each pick satisfying the equation above weight \\(y_i = \\frac{w_i}{\\sum w_i}\\), where \\(w_i = \\min(\\frac{1}{(n - i)^2}, 1)\\). Note that \\(w_i = 1\\) if and only if \\(n = i\\), and that \\(\\sum y_i  =1\\). Finally, we scale all the values so that the first pick is worth (roughly) 1000 points to allow for an easier comparison between models (this is also in line with how most NHL draft pick value models are structured).\n\n\nCode\nest_ps &lt;- rep(0, times = nrow(all_data_raw))\n\nfor(i in 1:nrow(all_data_raw)){\n  k &lt;- sqrt(i)\n  nearest &lt;- which(abs(seq(1, nrow(all_data_raw), 1) - i) &lt;= (k %/% 2)+1)\n  total_weight &lt;- sum(pmin(1/(i - nearest)^2, 1))\n  for(j in nearest){\n    weight &lt;- pmin(1/abs(i - j)^2, 1) / total_weight\n    est_ps[i] &lt;- est_ps[i] + weight * all_data_raw$mean_ps[j]\n  }\n}\n\nps_scale_fac &lt;- 1000 / est_ps[[1]]\n\nknn_raw &lt;- data.frame(overall = seq(1, length(est_ps), 1),\n                        value_ps = ps_scale_fac * est_ps)\n\n\n\nest_ps_prop &lt;- rep(0, times = nrow(all_data_prop))\n\nfor(i in 1:nrow(all_data_prop)){\n  k &lt;- sqrt(i)\n  nearest &lt;- which(abs(seq(1, nrow(all_data_prop), 1) - i) &lt;= (k %/% 2)+1)\n  total_weight &lt;- sum(pmin(1/abs(i - nearest)^2, 1))\n  for(j in nearest){\n    weight &lt;- pmin(1/(i - j)^2, 1) / total_weight\n    est_ps_prop[i] &lt;- est_ps_prop[i] + weight * all_data_prop$avg_prop_ps[j]\n  }\n}\n\nps_prop_scale_fac &lt;- 1000 / est_ps_prop[[1]]\n\nknn_prop &lt;- data.frame(overall = seq(1, length(est_ps_prop), 1),\n                        value_ps = ps_prop_scale_fac * est_ps_prop)\n\n\nNow that both the raw average method and average proportion method are on the same scale, we can plot them on top of each other. Indeed, they are similar before around pick 20 and nearly identical after that:\n\n\nCode\ncombined_data &lt;- rbind(mutate(knn_raw, mod = \"Raw PS\"), \n                       mutate(knn_prop, mod = \"Proportional\"))\nggplot(combined_data, aes(x = overall, y = value_ps, col = mod)) + \n  geom_point(alpha = 0.4) + \n  labs(title = \"Historical Value of Draft Picks by Overall\",\n       subtitle = \"Values smoothened using a weighted k-nearest-neigbour algorithm\", \n       x = \"Pick Number\", y = \"Value of Pick\", col = \"Model Used\")\n\n\n\n\n\n\n\n\n\nThough not perfect, this curve is much smoother than the previous one, so we will use this when fitting a curve.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transform</span>"
    ]
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "7  Model",
    "section": "",
    "text": "7.1 Setup\nWe install and load the necessary packages, along with functions from prior chapters and all_data.\nCode\n# install.packages(\"tidyverse\")\n# install.packages(\"dplyr\")\n# install.packages(\"gt\")\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(gt)\n\nsource(\"functions.R\") # load functions defined in prior chapters\n\nall_data &lt;- read.csv(\"all_data.csv\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model</span>"
    ]
  },
  {
    "objectID": "model.html#introduction",
    "href": "model.html#introduction",
    "title": "7  Model",
    "section": "7.2 Introduction",
    "text": "7.2 Introduction\nIn the model step, we will develop a model for predicting the value of picks in the NHL Entry Draft based on historical data. We will first start with a linear model, then progress to a logarithmic model, and then conclude by finding a model via non-linear regression.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model</span>"
    ]
  },
  {
    "objectID": "model.html#code",
    "href": "model.html#code",
    "title": "7  Model",
    "section": "7.3 Code",
    "text": "7.3 Code\n\n\nCode\nprop_knn_model &lt;- nls(value_ps ~ SSlogis(log(overall), phi1, phi2, phi3), \n                 data = knn_prop)\n\npred_data_prop_knn &lt;- data.frame(overall = seq(1, 224))\npred_vals_prop_knn &lt;- predict(prop_knn_model, pred_data_prop_knn)\n\npred_data &lt;- mutate(pred_data_prop_knn, value_ps = pred_vals_prop_knn)\n\nggplot(pred_data, aes(x = overall, y = value_ps)) + \n  geom_line(lwd = 1.5, col = \"red\") + \n  geom_point(data = knn_prop)\n\n\n\n\n\n\n\n\n\nCode\nraw_knn_model &lt;- nls(value_ps ~ SSlogis(log(overall), phi1, phi2, phi3), \n                 data = knn_raw)\n\npred_data_raw_knn &lt;- data.frame(overall = seq(1, 224))\npred_vals_raw_knn &lt;- predict(raw_knn_model, pred_data_raw_knn)\n\npred_data &lt;- mutate(pred_data, value_ps = pred_vals_raw_knn)\n\nggplot(pred_data, aes(x = overall, y = value_ps)) + \n  geom_line(lwd = 1.5, col = \"red\") + \n  geom_point(data = knn_raw)\n\n\n\n\n\n\n\n\n\n\n\nCode\nmean_data &lt;- all_data |&gt; \n  group_by(overall) |&gt; \n  summarize(mean_ps = mean(ps))\n\nraw_model &lt;- nls(mean_ps ~ SSlogis(log(overall), phi1, phi2, phi3), \n                 data = mean_data)\n\npred_data_raw &lt;- data.frame(overall = seq(1, 224))\npred_vals_raw &lt;- predict(raw_model, pred_data_raw)\n\npred_data_raw &lt;- mutate(pred_data_raw, value_ps = pred_vals_raw)\n\nggplot(pred_data, aes(x = overall, y = value_ps)) + \n  geom_line(lwd = 1.5, col = \"red\") + \n  geom_point(data = pred_data_raw)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model</span>"
    ]
  },
  {
    "objectID": "communicate.html",
    "href": "communicate.html",
    "title": "8  Communicate",
    "section": "",
    "text": "This is a placeholder for my communicate chapter and will eventually have plots, tables, etc.\nIn my RShiny app, I plan to allow users to enter potental trades, click “evaluate”, and then see the point gain/loss. Additionally, I’d like to put the point difference into context “this is equivalent to team A giving up the 174th overall pick”.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Communicate</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "ChatGPT Usage (will add link when done)\nValuation of NHL Draft Picks using Functional Data Analysis\nExamining the value of NHL Draft picks\nNHL draft: What does it cost to trade up?\nHockeyReference (for the data used in the report)\nR for Data Science (2e) (throughout the report)\nDevOps for Data Science (throughout the report)\nHappy Git and GitHub for the useR (throughout the report)\nMastering Shiny (for the RShiny app)\nNon-linear Regression in R (in the Model Chapter)\n\nFor now this is just a list of links. Later I will formalize it.",
    "crumbs": [
      "References"
    ]
  }
]