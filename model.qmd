---
title: "Model"
author: "Trevor S"
---

## Setup

We install and load the necessary packages, along with functions from prior chapters.

```{r,  message = FALSE, warning = FALSE}
# renv::install("patchwork")
# renv::install("stringr")

library(patchwork)
library(stringr)

source("functions.R") # load functions defined in prior chapters
```

## Introduction/Recap

Now that we have metrics representing different ways of calculating the historical value of a draft pick, we can now develop models for predicting the value of future picks. First, we will fit a linear regression model to the data, an then we will develop a model via non-linear regression. We will then put the models on the same scale by multiplying each predicted value by a constant, allowing us to compare models more effectively. As always, when fitting a model it is important to ensure that the underlying assumptions of a model hold, otherwise the model can be useless, or even worse, misleading. Note that the following two Stack Overflow posts were once again very helpful when writing the code in this chapter:

-   [Dynamic Variable naming in r](https://stackoverflow.com/questions/20855818/dynamic-variable-naming-in-r)

-   [Specifying column with its index rather than name](https://stackoverflow.com/questions/16187091/specifying-column-with-its-index-rather-than-name)

Recall the four plots we ended the Transform chapter with, based on the mean PS, mean GP, success rate, and mean adjusted PS for every selection between 1 and 224. For convenience we replot this data below:

```{r}
(plot_mean_ps + plot_mean_gp) / (plot_suc_rate + plot_mean_adj_ps)
```

## Linear Regression

We use `lm` to fit a linear model to each of the metrics:

```{r}
metrics <- c("mean_ps", "mean_gp", "suc_rate", "mean_adj_ps")
overall <- all_data_comb$overall

lin_models <- lapply(metrics, \(x) lm(all_data_comb[[x]] ~ overall))
```

For each linear model, we generate a vector of predicted values and plot it on top of the historical values:

```{r}
pred_vals <- lapply(seq(1,4), 
                      \(x) predict(lin_models[[x]], 
                                   data.frame(overall = seq(1,224))))

pred_vals <- data.frame(overall = seq(1, 224), 
                          mean_ps = pred_vals[[1]], 
                          mean_gp = pred_vals[[2]], 
                          suc_rate = pred_vals[[3]], 
                          mean_adj_ps = pred_vals[[4]])

names <- c("Mean PS", "Mean GP", "Success Rate", "Mean Adjusted PS")

for(i in 1:length(metrics)){
  assign(str_glue("plot_lm_{metrics[i]}"), 
         get(str_glue("plot_{metrics[i]}")) + 
           geom_line(data = pred_vals, aes_string(x = "overall", y = metrics[i]), 
                     col = "red", lwd = 1.5))
  }

(plot_lm_mean_ps + plot_lm_mean_gp) / (plot_lm_suc_rate + plot_lm_mean_adj_ps)
```

All four of these linear models are very clearly inadequate, but for sake of completeness we will look at plots of the residuals vs fitted values.

```{r}
resids <- lapply(lin_models, residuals)

resids <- data.frame(mean_ps_resid = resids[[1]], 
                     mean_gp_resid = resids[[2]], 
                     suc_rate_resid = resids[[3]], 
                     mean_adj_ps_resid = resids[[4]])

pred_vals_resid <- cbind(pred_vals, resids)

for(i in 1:length(metrics)){
  assign(str_glue("plot_res_{metrics[i]}"), 
         ggplot(pred_vals_resid, aes_string(x = str_glue("{metrics[i]}"), 
                                            y = str_glue("{metrics[i]}_resid"))) + 
           geom_hline(yintercept = 0, col = "red", lwd = 1.5) + geom_point())
  }

(plot_res_mean_ps + plot_res_mean_gp) / (plot_res_suc_rate + plot_res_mean_adj_ps)
```

Clearly none of these models are appropriate, they very clearly fail the assumptions regarding a correct functional form, constant variance of residuals, and independent residual assumptions. It is also clear that the errors are not normally distributed.

## Non-Linear Regression

Given that none of the four linear models were appropriate, we will reattempt to fit a model using non-linear regression (ie the `nls` function, which stands for non-linear least squares). The resource [Non-linear Regression in R](https://tuos-bio-data-skills.github.io/intro-stats-book/non-linear-regression-in-R.html) was very helpful when working on this section. In short, we will be fitting the model

$$
v_i = \frac{\phi_{1, m}}{1+e^{(\phi_{2,m}- i)/\phi_{3,m}}}
$$

Where

-   $v_{i,m}$ is the value of pick $i$ based on metric $m$.

-   $\phi_{1, m},\phi_{2, m},\phi_{3, m}$ are parameters that depend on which metric we are using.

We fit this model as follows:

## After this point I was lazy about coding smartly and writing stuff

```{r}
ps_nls <- nls(mean_ps ~ SSlogis(log(overall), phi1, phi2, phi3), 
                 data = all_data_comb)
gp_nls <- nls(mean_gp ~ SSlogis(log(overall), phi1, phi2, phi3), 
                 data = all_data_comb)
sr_nls <- nls(suc_rate ~ SSlogis(log(overall), phi1, phi2, phi3), 
                 data = all_data_comb)
aps_nls <- nls(mean_adj_ps ~ SSlogis(log(overall), phi1, phi2, phi3), 
                 data = all_data_comb)

pred_vals <- data.frame(pick = seq(1, 224), 
                        pred_mean_ps = predict(ps_nls, pick), 
                        pred_mean_gp = predict(gp_nls, pick), 
                        pred_suc_rate = predict(sr_nls, pick), 
                        pred_mean_adj_ps = predict(aps_nls, pick)) 

nls_ps_plot <- plot_mean_ps + geom_line(data = pred_vals, 
                    aes(x = overall, y = pred_mean_ps), col = "red", lwd = 1.5)
nls_gp_plot <- plot_mean_gp + geom_line(data = pred_vals, 
                    aes(x = overall, y = pred_mean_gp), col = "red", lwd = 1.5)
nls_sr_plot <- plot_suc_rate + geom_line(data = pred_vals, 
                    aes(x = overall, y = pred_suc_rate), col = "red", lwd = 1.5)
nls_aps_plot <- plot_mean_adj_ps + geom_line(data = pred_vals, 
                    aes(x = overall, y = pred_mean_adj_ps), col = "red", lwd = 1.5)

(nls_ps_plot + nls_gp_plot) / (nls_sr_plot + nls_aps_plot)
```

## One Last Adjustment

Scale $v_1$ to 1000.

## Model Selection

Probably will take the model with the lowest RSS.
