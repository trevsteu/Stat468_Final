---
title: "holding_file"
---

## Setup

```{r}
source("functions.R")
```

## Visualize

## Transform

This will result in older drafts receiving more weight when we take the average of PS, which is unideal because we'd like all drafts to be equally weighted. To give more weight to more recent drafts, we will change our metric *slightly* for active players selected in drafts after 2015, and will instead use $p^{mod}_{ij} = \frac{ps_{ij}}{\sum_i ps_{ij}}C$, where $ps_{ij}$ is the PS of the player picked at selection $i$ in year $j$ and $C$ is the mean total point share of all drafts between 1996 and 2015. In other words, if draft $j$ were to be an average draft, and all players continued generating PS at their current rates, then player $i$ from draft $j$ would finish their career with a PS of $p^{mod}_{ij}$ To reiterate, this is still not a perfect fix because "late bloomers" drafted after 2015 may not have made any contributions yet, even if they will later on. This adjustment benefits active players drafted in 2016-2020 who have already made NHL contributions, but will make no difference for those who have not yet made the NHL. There is no reason to make this adjustment for players drafted between 2016 and 2020 who have retired.

```{r}
C <- sum(filter(all_data, year <= 2015)$ps) /
  length(unique(filter(all_data, year <= 2015)$year))

all_data_new <- all_data |> 
  filter(year > 2015 & to == 2025) |> # active players drafted after 2015
  group_by(year) |> 
  mutate(ps = ps/sum(ps)*C)

all_data_old <- all_data |> 
  filter(year <= 2015 | to != 2025) # complement of the previous set

all_data_mod <- rbind(all_data_old, all_data_new)

all_data_mod
```

To reiterate, this is not a perfect solution because players drafted in 2015 and earlier . However estimating the remaining value of a player's career could be a project all on its own, so we will stop here with this proxy.

Note that usually it is not advisable to take the mean of a bunch of ratios because it gives each ratio an equal weight. However, here it is appropriate because we want to give each draft an equal weight. We will recreate the same plot, but note that the only variation will come from the number of selections in a draft (since what we are plotting is literally $\frac{1}{\text{number of picks in draft}}$). Additionally, the scale on the $y$-axis shows that these values are basically all the same.

```{r}
all_data_prop |> 
  group_by(year) |> 
  summarize(avg_ps = mean(prop_ps)) |>
  ggplot(aes(x = year, y = avg_ps)) +
  geom_point()
```

In case this approach is not appropriate, we will still fit a model using the raw `ps` values in the Model step, it turns out the models will be quite similar.

```{r}
all_data_prop <- all_data_prop |> 
  group_by(overall) |>  
  summarize(avg_prop_ps = mean(prop_ps),
            .groups = "drop")

all_data_prop
```

Finally, we utilize a weighted $k$-nearest neighbour algorithm which will help smoothen out the plot from the end of the visualization chapter. As a reminder, here is that plot:

```{r, warning = FALSE}
all_data_raw <- all_data |> 
  group_by(overall) |> 
  summarize(mean_ps = mean(ps)) 

head(all_data_raw, 10)

ggplot(all_data_raw, aes(x = overall, y = mean_ps)) + 
  geom_point() +
  geom_point(aes(x = 205, y = mean(filter(all_data, overall==205)$ps)), col = "salmon") +
  labs(title = "Mean PS by Pick Number",
       x = "Pick Number", y = "Mean PS") + 
  annotate(geom = "segment", x = 175, y = 37.5, xend = 203, yend = 14, colour = "salmon",
    arrow = arrow(type = "open", length = unit(0.32, "cm"))) +
  annotate(geom = "label", x = 140, y = 39,
    label = "205th overall selection,\n(mean PS of 13.124)",
    hjust = "left", colour = "salmon")
```

First, we will recreate this plot using the `avg_prop_ps` values we just defined:

```{r, warning = FALSE}
all_data_prop |> 
  ggplot(aes(x = overall, y = avg_prop_ps)) + 
  geom_point() +
  geom_point(aes(x = 205, y = mean(filter(all_data_prop, overall==205)$avg_prop_ps)), 
             col = "salmon") +
  labs(title = "Mean Proportion of PS by Pick Number",
       x = "Pick Number", y = "Mean Proportion of PS") + 
  annotate(geom = "segment", x = 175, y = 0.015, xend = 203, yend = 0.0065, colour = "salmon",
    arrow = arrow(type = "open", length = unit(0.32, "cm"))) +
  annotate(geom = "label", x = 140, y = 0.018,
    label = "205th overall selection, (mean\n0.609% of the draft's ps)",
    hjust = "left", colour = "salmon")

```

The shapes of the two graphs are quite similar, but the outliers in the second one seem slighly less egregious.

## Model

### Code

```{r, eval=FALSE}
prop_knn_model <- nls(value_ps ~ SSlogis(log(overall), phi1, phi2, phi3), 
                 data = knn_prop)

pred_data_prop_knn <- data.frame(overall = seq(1, 224))
pred_vals_prop_knn <- predict(prop_knn_model, pred_data_prop_knn)

pred_data <- mutate(pred_data_prop_knn, value_ps = pred_vals_prop_knn)

ggplot(pred_data, aes(x = overall, y = value_ps)) + 
  geom_line(lwd = 1.5, col = "red") + 
  geom_point(data = knn_prop)



raw_knn_model <- nls(value_ps ~ SSlogis(log(overall), phi1, phi2, phi3), 
                 data = knn_raw)

pred_data_raw_knn <- data.frame(overall = seq(1, 224))
pred_vals_raw_knn <- predict(raw_knn_model, pred_data_raw_knn)

pred_data <- mutate(pred_data, value_ps = pred_vals_raw_knn)

ggplot(pred_data, aes(x = overall, y = value_ps)) + 
  geom_line(lwd = 1.5, col = "red") + 
  geom_point(data = knn_raw)
```

```{r, eval=FALSE}
mean_data <- all_data |> 
  group_by(overall) |> 
  summarize(mean_ps = mean(ps))

raw_model <- nls(mean_ps ~ SSlogis(log(overall), phi1, phi2, phi3), 
                 data = mean_data)

pred_data_raw <- data.frame(overall = seq(1, 224))
pred_vals_raw <- predict(raw_model, pred_data_raw)

pred_data_raw <- mutate(pred_data_raw, value_ps = pred_vals_raw)

ggplot(pred_data, aes(x = overall, y = value_ps)) + 
  geom_line(lwd = 1.5, col = "red") + 
  geom_point(data = pred_data_raw)
```

### KNN Stuff

We now apply the weighted $k$-nearest neighbour algorithm, first on `all_data`, and then on `all_data_prop`. Note that we will choose $k$ to be a function of $n$, specifically we will include any pick which is within $\lfloor \frac{\sqrt n}{2}\rfloor +1$ of $n$ which means that our estimate of $v_1$ depends on the historical values of just pick 1 and 2, whereas the estimated value of pick 200 depends on the historical values of picks 192-208 (ie any pick $i$ such that $| i - 200 | \le \lfloor \frac{\sqrt {200}}{2}\rfloor +1 = 8$). We also need to choose a weight function, we choose to give each pick satisfying the equation above weight $y_i = \frac{w_i}{\sum w_i}$, where $w_i = \min(\frac{1}{(n - i)^2}, 1)$. Note that $w_i = 1$ if and only if $n = i$, and that $\sum y_i  =1$. Finally, we scale all the values so that the first pick is worth (roughly) 1000 points to allow for an easier comparison between models (this is also in line with how most NHL draft pick value models are structured).

```{r, eval=FALSE}
est_ps <- rep(0, times = nrow(all_data_raw))

for(i in 1:nrow(all_data_raw)){
  k <- sqrt(i)
  nearest <- which(abs(seq(1, nrow(all_data_raw), 1) - i) <= (k %/% 2)+1)
  total_weight <- sum(pmin(1/(i - nearest)^2, 1))
  for(j in nearest){
    weight <- pmin(1/abs(i - j)^2, 1) / total_weight
    est_ps[i] <- est_ps[i] + weight * all_data_raw$mean_ps[j]
  }
}

ps_scale_fac <- 1000 / est_ps[[1]]

knn_raw <- data.frame(overall = seq(1, length(est_ps), 1),
                        value_ps = ps_scale_fac * est_ps)



est_ps_prop <- rep(0, times = nrow(all_data_prop))

for(i in 1:nrow(all_data_prop)){
  k <- sqrt(i)
  nearest <- which(abs(seq(1, nrow(all_data_prop), 1) - i) <= (k %/% 2)+1)
  total_weight <- sum(pmin(1/abs(i - nearest)^2, 1))
  for(j in nearest){
    weight <- pmin(1/(i - j)^2, 1) / total_weight
    est_ps_prop[i] <- est_ps_prop[i] + weight * all_data_prop$avg_prop_ps[j]
  }
}

ps_prop_scale_fac <- 1000 / est_ps_prop[[1]]

knn_prop <- data.frame(overall = seq(1, length(est_ps_prop), 1),
                        value_ps = ps_prop_scale_fac * est_ps_prop)
```

Now that both the raw average method and average proportion method are on the same scale, we can plot them on top of each other. Indeed, they are similar before around pick 20 and nearly identical after that:

```{r, eval=FALSE}
combined_data <- rbind(mutate(knn_raw, mod = "Raw PS"), 
                       mutate(knn_prop, mod = "Proportional"))
ggplot(combined_data, aes(x = overall, y = value_ps, col = mod)) + 
  geom_point(alpha = 0.4) + 
  labs(title = "Historical Value of Draft Picks by Overall",
       subtitle = "Values smoothened using a weighted k-nearest-neigbour algorithm", 
       x = "Pick Number", y = "Value of Pick", col = "Model Used")
```

Though not perfect, this curve is much smoother than the previous one, so we will use this when fitting a curve.

# 
