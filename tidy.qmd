---
title: "Tidy"
author: "Trevor S"
---

## Setup

We install and load the necessary packages, along with functions from prior chapters.

```{r, message = FALSE, warning = FALSE}
# renv::install("rvest")
# renv::install("tidyverse")
# renv::install("gt")
# renv::install("DBI")
# renv::install("duckdb")
# renv::install("aws.s3")
# renv::install("paws")
# renv::install("arrow")

library(rvest)
library(tidyverse)
library(stringr)
library(gt)
library(DBI)
library(duckdb)
library(aws.s3)
library(paws)
library(arrow)

source("functions.R") # load functions defined in prior chapters
```

## Introduction

In the Tidy chapter, we will put the data into tidy form, clean it, and store it in an S3 bucket, which will make our analysis in the following chapters easier. Despite the table from the previous chapter *looking* fairly clean, further inspection reveals some issues, as shown in Figure 4.2.1:

```{r, warning = FALSE}
start_year <- 1996
end_year <- 2020

import_draft(start_year)[23:30,] |> # picks 23-28
  gt() |> 
  opt_all_caps() |> 
  tab_source_note("Table 4.2.1: Some Issues in our Data")
```

Some problems that immediately come up are:

-   Two rows get inserted at the end of every round to indicate the round changed.

-   Though not visible because of the usage of `gt`, numbers are being coded as strings (`overall`, `age`, `to`, etc).

-   (At least) one player is missing everything except for their pick number, name, team, position, nationality, and amateur team.

-   The `+/-` column got renamed to `x`.

-   There were two `gp` columns, one got automatically renamed to `gp2` when we used `janitor::clean_names()` in `import_draft`.

By doing a little bit of detective work with some of the other players with missing values elsewhere in the dataset, we notice that players who never played in the NHL have empty strings listed for everything except for their pick number, name, team, position, nationality, and amateur team. This is one of several things we will do in the Tidy chapter to prepare our data for analysis.

## Cleaning

We build a function to tidy the data. In particular, we want it to:

-   Remove the rows added between rounds.

-   Correct the types of each column so we can use numeric columns in calculations.

-   Change `gp` and `ps` values to `0` for players who never played in the NHL or have a negative `ps`.

-   Standardize position to either be F (forward, any combination of LW, RW, C), D (defenceman), or G (goalie). Note that Kaspars Astashenko has his position listed as "D/W", a quick search of the [NHL website](https://www.nhl.com/player/kaspars-astashenko-8468000) reveals he was a defenceman.

-   If `is.na(to)`, then the player never played in the NHL, so set it to the draft year.

-   Add a `year` column so we can combine all of the data into a single data frame.

-   Select the columns we care about (`year`, `overall`, `pos`, `to`, `ps`, and `gp`) in that order.

-   Remove any players selected after 224$^{\text{th}}$ overall (the number of picks in the 2025 NHL Entry Draft).

-   We don't care about `+/-`, so we can ignore the column being renamed since we won't be using it anyway.

-   It also turns out that for skaters `gp2` is empty and for goalies `gp` and `gp2` will have the same value, so this issue can be resolved by simply selecting `gp`.

-   The 69$^{\text{th}}$ pick of 2011 was forfeited and and 123$^{\text{rd}}$ pick of 2002 was invalid; both are listed as blank rows, so these should be removed.

Note that we cannot remove the round separating rows by removing a specified row number since many of the drafts in our dataset have different numbers of picks per round, and some rounds within the same draft have even had a different numbers of picks.

```{r, warning = FALSE, message = FALSE}
tidy_draft <- function(year){
  draft_year_table <- import_draft(year) |> # get the untidied data
    filter(overall != "Overall" & overall != "" & # remove extra rows
             as.numeric(overall) < 225 & # remove players drafted after pick 224
             amateur_team != "()") |> # remove invalid/forfeited picks 
    type_convert() |> # fix types 
    mutate("year" = year, 
           "ps" = pmax(coalesce(ps, 0), 0), "gp" = coalesce(gp, 0), # players who never played in the NHL
           "to" = coalesce(to, year), # players who never played in the NHL
           "pos" = ifelse(str_count(pos, "G") == 1, "G", # fix positions
                          ifelse(str_count(pos, "D") == 1, "D", "F"))) |> 
    select(year, overall, to, pos, ps, gp) # columns we care about
  draft_year_table
}

tidy_draft(1996) |> 
  head(50) |> 
  gt() |> 
  opt_all_caps() |> 
  tab_source_note("Table 4.3.1: The Function Works Correctly")
```

Figure 4.3.1 confirms the function completes the tasks listed at the start of this section. We can now proceed to import the data, we choose to clean it as we import it.

## Getting the Data

We now load in all of the data and use `rbind()` to bind the tables together, giving us a single data frame to work with. This also means that our data will be in a mix of long and wide format, since the `year` and `overall` columns are formatted like they are in long format but the `to`, `pos`, `gp`, and `ps` columns are the same as they would be in a wide format. Note that this function takes 2-3 minutes to run (because of the `Sys.sleep(5)` line, which is necessary to prevent us getting rate limited). We will soon store the data in a S3 bucket so we don't have to run this function any more than we have to.

```{r, warning=FALSE, message=FALSE, eval = FALSE}
all_data <- do.call(rbind, lapply(seq(start_year, end_year, 1),
                   \(x) tidy_draft(x)))
```

```{r, message=FALSE, echo=FALSE, include=FALSE, warning=FALSE}
con <- dbConnect(duckdb())

dbExecute(con, "INSTALL httpfs;")
dbExecute(con, "LOAD httpfs;")

all_data_test <- dbGetQuery(con, "SELECT * 
                            FROM read_parquet('s3://trevor-stat468/all_data.parquet');")

# I'm cheating here by loading from the bucket because I don't want to run
#   the chunk above since tidy_draft takes such a long time to run and render
```

## Verification

We can check that we loaded the data correctly, there should be between 5250 and 5600 rows (the number of picks in a draft has changed over the years in our dataset as we will see, but is always between 210 and 224 so 25 drafts will be between ) and 6 columns:

```{r}
dim(all_data) # confirm there are 5250-5600 rows and 6 columns

length(unique(all_data$year)) # confirm all 25 years have been included

# should be the last 10 picks from 2020, note that there were 7*31 = 217 
#   picks in the draft that year
all_data |>
  tail(10) |>
  gt() |> 
  opt_all_caps() |> 
  tab_source_note("Table 4.4.1: The Last 10 Picks from our Dataset")

all_data |> # Confirm pick 123 of 2002 was removed (it was an invalid pick)
  filter(year == 2002 & overall <= 125 & overall >= 121) |> 
  gt() |> 
  opt_all_caps() |> 
  tab_source_note("Table: 4.4.2: Pick 123 in 2002 was Removed")
```

These checks all returned what they should.

## Storing the Data

Finally, recall that the `import_data` function (and thus the `tidy_draft` function) both take quite a while to run, especially when importing data for a large number of years. To resolve this issue, we will store the data in an AWS S3 bucket so future chapters can get the data from the S3 bucket instead of from Hockey Reference, making this report render significantly faster. We do this by following the instructions given by @agarwal_how_2020, which tell us to save `all_data` in parquet file and then write it to our S3 bucket.

```{r}
Sys.setenv("AWS_ACCESS_KEY_ID" = Sys.getenv("AWS_ACCESS_KEY_ID"),
           "AWS_SECRET_ACCESS_KEY" = Sys.getenv("AWS_SECRET_ACCESS_KEY"), 
           "AWS_DEFAULT_REGION" = "us-east-2")
bucket = "trevor-stat468"

s3write_using(all_data, FUN = write_parquet, 
              bucket = bucket, object = "all_data.parquet")
```

In this and future chapters, we still start by loading in all of the functions from `functions.R`, which includes all the functions and global constants defined in all chapters, in addition to querying the data from the AWS S3 bucket using duckdb. `functions.R` is included in this project's GitHub repo, and the part of it that loads `all_data` is also given below (here we load it into `all_data_test` to show that it actually works). I also followed some of the instructions from @michonneau_how_2023 to help me write the below code.

```{r, messages = FALSE, warning = FALSE, results= 'hide'}
con <- dbConnect(duckdb())

dbExecute(con, "INSTALL httpfs;")
dbExecute(con, "LOAD httpfs;")

all_data_test <- dbGetQuery(con, "SELECT * 
                            FROM read_parquet('s3://trevor-stat468/all_data.parquet');")

DBI::dbDisconnect(con)
```

```{r}
dim(all_data_test)
all_data_test |> 
  head(10) |> 
  gt() |>
  opt_all_caps() |> 
  tab_source_note("Table 4.6.1: Data Loaded from S3 Bucket")
```

Figure 4.6.1 confirms that the data can be loaded from the S3 bucket. In future chapters we will edit this data and will put the new version in the same S3 bucket. We now proceed to the Visualize chapter.
